<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <title>datadr R Package Documentation</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="">
    <meta name="author" content="">

    <!-- Le styles -->
    <link href="bootstrap/css/bootstrap.css" rel="stylesheet">
    <link href="css/pygment.css" rel="stylesheet">
    
    <style type="text/css">
      body {
        padding-top: 20px;
        padding-bottom: 40px;
      }

      /* Custom container */
      .container-narrow {
        margin: 0 auto;
        max-width: 900px;
      }
      .container-narrow > hr {
        margin: 15px 0 20px 0;
      }
      
      #next {
         font-size: 14px;
      }

      #previous {
         font-size: 14px;
      }

      .fref_title {
         border-bottom:1px solid #EEE;
      }

      .myHeader {
         font-family: 'Chivo', 'Helvetica Neue', Helvetica, Arial, serif; font-weight: 400;
         letter-spacing: -1px;
         font-size: 28px;
         line-height: 40px;
         color: #d14;
         text-shadow: 8px 2px 6 rgba(55, 55, 55, 0.5);
      }

      .unselectable {
         -moz-user-select: none;
         -webkit-user-select: none;
         -ms-user-select: none;
      }

/*      #main-content {
         margin-top: -15px;
      }
*/
    </style>
        <style type="text/css">
       pre .operator,
       pre .paren { color: #555555 }

       pre .literal {
         color: rgb(88, 72, 246); font-weight: bold;
       }
/*       pre .literal { color: #006699; font-weight: bold } */

       pre .number { color: #FF6600 }
/*       pre .comment { color: #0099FF; font-style: italic }*/
       pre .comment { color: #999; font-style: italic }
       pre .keyword { color: #006699; font-weight: bold }

       pre .identifier {
         color: rgb(0, 0, 0);
       }

       pre .string { color: #CC3300 }
    </style>

    <!-- R syntax highlighter -->
    <script type="text/javascript">
    var hljs=new function(){function m(p){return p.replace(/&/gm,"&amp;").replace(/</gm,"&lt;")}function f(r,q,p){return RegExp(q,"m"+(r.cI?"i":"")+(p?"g":""))}function b(r){for(var p=0;p<r.childNodes.length;p++){var q=r.childNodes[p];if(q.nodeName=="CODE"){return q}if(!(q.nodeType==3&&q.nodeValue.match(/\s+/))){break}}}function h(t,s){var p="";for(var r=0;r<t.childNodes.length;r++){if(t.childNodes[r].nodeType==3){var q=t.childNodes[r].nodeValue;if(s){q=q.replace(/\n/g,"")}p+=q}else{if(t.childNodes[r].nodeName=="BR"){p+="\n"}else{p+=h(t.childNodes[r])}}}if(/MSIE [678]/.test(navigator.userAgent)){p=p.replace(/\r/g,"\n")}return p}function a(s){var r=s.className.split(/\s+/);r=r.concat(s.parentNode.className.split(/\s+/));for(var q=0;q<r.length;q++){var p=r[q].replace(/^language-/,"");if(e[p]){return p}}}function c(q){var p=[];(function(s,t){for(var r=0;r<s.childNodes.length;r++){if(s.childNodes[r].nodeType==3){t+=s.childNodes[r].nodeValue.length}else{if(s.childNodes[r].nodeName=="BR"){t+=1}else{if(s.childNodes[r].nodeType==1){p.push({event:"start",offset:t,node:s.childNodes[r]});t=arguments.callee(s.childNodes[r],t);p.push({event:"stop",offset:t,node:s.childNodes[r]})}}}}return t})(q,0);return p}function k(y,w,x){var q=0;var z="";var s=[];function u(){if(y.length&&w.length){if(y[0].offset!=w[0].offset){return(y[0].offset<w[0].offset)?y:w}else{return w[0].event=="start"?y:w}}else{return y.length?y:w}}function t(D){var A="<"+D.nodeName.toLowerCase();for(var B=0;B<D.attributes.length;B++){var C=D.attributes[B];A+=" "+C.nodeName.toLowerCase();if(C.value!==undefined&&C.value!==false&&C.value!==null){A+='="'+m(C.value)+'"'}}return A+">"}while(y.length||w.length){var v=u().splice(0,1)[0];z+=m(x.substr(q,v.offset-q));q=v.offset;if(v.event=="start"){z+=t(v.node);s.push(v.node)}else{if(v.event=="stop"){var p,r=s.length;do{r--;p=s[r];z+=("</"+p.nodeName.toLowerCase()+">")}while(p!=v.node);s.splice(r,1);while(r<s.length){z+=t(s[r]);r++}}}}return z+m(x.substr(q))}function j(){function q(x,y,v){if(x.compiled){return}var u;var s=[];if(x.k){x.lR=f(y,x.l||hljs.IR,true);for(var w in x.k){if(!x.k.hasOwnProperty(w)){continue}if(x.k[w] instanceof Object){u=x.k[w]}else{u=x.k;w="keyword"}for(var r in u){if(!u.hasOwnProperty(r)){continue}x.k[r]=[w,u[r]];s.push(r)}}}if(!v){if(x.bWK){x.b="\\b("+s.join("|")+")\\s"}x.bR=f(y,x.b?x.b:"\\B|\\b");if(!x.e&&!x.eW){x.e="\\B|\\b"}if(x.e){x.eR=f(y,x.e)}}if(x.i){x.iR=f(y,x.i)}if(x.r===undefined){x.r=1}if(!x.c){x.c=[]}x.compiled=true;for(var t=0;t<x.c.length;t++){if(x.c[t]=="self"){x.c[t]=x}q(x.c[t],y,false)}if(x.starts){q(x.starts,y,false)}}for(var p in e){if(!e.hasOwnProperty(p)){continue}q(e[p].dM,e[p],true)}}function d(B,C){if(!j.called){j();j.called=true}function q(r,M){for(var L=0;L<M.c.length;L++){if((M.c[L].bR.exec(r)||[null])[0]==r){return M.c[L]}}}function v(L,r){if(D[L].e&&D[L].eR.test(r)){return 1}if(D[L].eW){var M=v(L-1,r);return M?M+1:0}return 0}function w(r,L){return L.i&&L.iR.test(r)}function K(N,O){var M=[];for(var L=0;L<N.c.length;L++){M.push(N.c[L].b)}var r=D.length-1;do{if(D[r].e){M.push(D[r].e)}r--}while(D[r+1].eW);if(N.i){M.push(N.i)}return f(O,M.join("|"),true)}function p(M,L){var N=D[D.length-1];if(!N.t){N.t=K(N,E)}N.t.lastIndex=L;var r=N.t.exec(M);return r?[M.substr(L,r.index-L),r[0],false]:[M.substr(L),"",true]}function z(N,r){var L=E.cI?r[0].toLowerCase():r[0];var M=N.k[L];if(M&&M instanceof Array){return M}return false}function F(L,P){L=m(L);if(!P.k){return L}var r="";var O=0;P.lR.lastIndex=0;var M=P.lR.exec(L);while(M){r+=L.substr(O,M.index-O);var N=z(P,M);if(N){x+=N[1];r+='<span class="'+N[0]+'">'+M[0]+"</span>"}else{r+=M[0]}O=P.lR.lastIndex;M=P.lR.exec(L)}return r+L.substr(O,L.length-O)}function J(L,M){if(M.sL&&e[M.sL]){var r=d(M.sL,L);x+=r.keyword_count;return r.value}else{return F(L,M)}}function I(M,r){var L=M.cN?'<span class="'+M.cN+'">':"";if(M.rB){y+=L;M.buffer=""}else{if(M.eB){y+=m(r)+L;M.buffer=""}else{y+=L;M.buffer=r}}D.push(M);A+=M.r}function G(N,M,Q){var R=D[D.length-1];if(Q){y+=J(R.buffer+N,R);return false}var P=q(M,R);if(P){y+=J(R.buffer+N,R);I(P,M);return P.rB}var L=v(D.length-1,M);if(L){var O=R.cN?"</span>":"";if(R.rE){y+=J(R.buffer+N,R)+O}else{if(R.eE){y+=J(R.buffer+N,R)+O+m(M)}else{y+=J(R.buffer+N+M,R)+O}}while(L>1){O=D[D.length-2].cN?"</span>":"";y+=O;L--;D.length--}var r=D[D.length-1];D.length--;D[D.length-1].buffer="";if(r.starts){I(r.starts,"")}return R.rE}if(w(M,R)){throw"Illegal"}}var E=e[B];var D=[E.dM];var A=0;var x=0;var y="";try{var s,u=0;E.dM.buffer="";do{s=p(C,u);var t=G(s[0],s[1],s[2]);u+=s[0].length;if(!t){u+=s[1].length}}while(!s[2]);if(D.length>1){throw"Illegal"}return{r:A,keyword_count:x,value:y}}catch(H){if(H=="Illegal"){return{r:0,keyword_count:0,value:m(C)}}else{throw H}}}function g(t){var p={keyword_count:0,r:0,value:m(t)};var r=p;for(var q in e){if(!e.hasOwnProperty(q)){continue}var s=d(q,t);s.language=q;if(s.keyword_count+s.r>r.keyword_count+r.r){r=s}if(s.keyword_count+s.r>p.keyword_count+p.r){r=p;p=s}}if(r.language){p.second_best=r}return p}function i(r,q,p){if(q){r=r.replace(/^((<[^>]+>|\t)+)/gm,function(t,w,v,u){return w.replace(/\t/g,q)})}if(p){r=r.replace(/\n/g,"<br>")}return r}function n(t,w,r){var x=h(t,r);var v=a(t);var y,s;if(v){y=d(v,x)}else{return}var q=c(t);if(q.length){s=document.createElement("pre");s.innerHTML=y.value;y.value=k(q,c(s),x)}y.value=i(y.value,w,r);var u=t.className;if(!u.match("(\\s|^)(language-)?"+v+"(\\s|$)")){u=u?(u+" "+v):v}if(/MSIE [678]/.test(navigator.userAgent)&&t.tagName=="CODE"&&t.parentNode.tagName=="PRE"){s=t.parentNode;var p=document.createElement("div");p.innerHTML="<pre><code>"+y.value+"</code></pre>";t=p.firstChild.firstChild;p.firstChild.cN=s.cN;s.parentNode.replaceChild(p.firstChild,s)}else{t.innerHTML=y.value}t.className=u;t.result={language:v,kw:y.keyword_count,re:y.r};if(y.second_best){t.second_best={language:y.second_best.language,kw:y.second_best.keyword_count,re:y.second_best.r}}}function o(){if(o.called){return}o.called=true;var r=document.getElementsByTagName("pre");for(var p=0;p<r.length;p++){var q=b(r[p]);if(q){n(q,hljs.tabReplace)}}}function l(){if(window.addEventListener){window.addEventListener("DOMContentLoaded",o,false);window.addEventListener("load",o,false)}else{if(window.attachEvent){window.attachEvent("onload",o)}else{window.onload=o}}}var e={};this.LANGUAGES=e;this.highlight=d;this.highlightAuto=g;this.fixMarkup=i;this.highlightBlock=n;this.initHighlighting=o;this.initHighlightingOnLoad=l;this.IR="[a-zA-Z][a-zA-Z0-9_]*";this.UIR="[a-zA-Z_][a-zA-Z0-9_]*";this.NR="\\b\\d+(\\.\\d+)?";this.CNR="\\b(0[xX][a-fA-F0-9]+|(\\d+(\\.\\d*)?|\\.\\d+)([eE][-+]?\\d+)?)";this.BNR="\\b(0b[01]+)";this.RSR="!|!=|!==|%|%=|&|&&|&=|\\*|\\*=|\\+|\\+=|,|\\.|-|-=|/|/=|:|;|<|<<|<<=|<=|=|==|===|>|>=|>>|>>=|>>>|>>>=|\\?|\\[|\\{|\\(|\\^|\\^=|\\||\\|=|\\|\\||~";this.ER="(?![\\s\\S])";this.BE={b:"\\\\.",r:0};this.ASM={cN:"string",b:"'",e:"'",i:"\\n",c:[this.BE],r:0};this.QSM={cN:"string",b:'"',e:'"',i:"\\n",c:[this.BE],r:0};this.CLCM={cN:"comment",b:"//",e:"$"};this.CBLCLM={cN:"comment",b:"/\\*",e:"\\*/"};this.HCM={cN:"comment",b:"#",e:"$"};this.NM={cN:"number",b:this.NR,r:0};this.CNM={cN:"number",b:this.CNR,r:0};this.BNM={cN:"number",b:this.BNR,r:0};this.inherit=function(r,s){var p={};for(var q in r){p[q]=r[q]}if(s){for(var q in s){p[q]=s[q]}}return p}}();hljs.LANGUAGES.r={dM:{c:[hljs.HCM,{cN:"number",b:"\\b0[xX][0-9a-fA-F]+[Li]?\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"number",b:"\\b\\d+(?:[eE][+\\-]?\\d*)?L\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"number",b:"\\b\\d+\\.(?!\\d)(?:i\\b)?",e:hljs.IMMEDIATE_RE,r:1},{cN:"number",b:"\\b\\d+(?:\\.\\d*)?(?:[eE][+\\-]?\\d*)?i?\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"number",b:"\\.\\d+(?:[eE][+\\-]?\\d*)?i?\\b",e:hljs.IMMEDIATE_RE,r:1},{cN:"keyword",b:"(?:tryCatch|library|setGeneric|setGroupGeneric)\\b",e:hljs.IMMEDIATE_RE,r:10},{cN:"keyword",b:"\\.\\.\\.",e:hljs.IMMEDIATE_RE,r:10},{cN:"keyword",b:"\\.\\.\\d+(?![\\w.])",e:hljs.IMMEDIATE_RE,r:10},{cN:"keyword",b:"\\b(?:function)",e:hljs.IMMEDIATE_RE,r:2},{cN:"keyword",b:"(?:if|in|break|next|repeat|else|for|return|switch|while|try|stop|warning|require|attach|detach|source|setMethod|setClass)\\b",e:hljs.IMMEDIATE_RE,r:1},{cN:"literal",b:"(?:NA|NA_integer_|NA_real_|NA_character_|NA_complex_)\\b",e:hljs.IMMEDIATE_RE,r:10},{cN:"literal",b:"(?:NULL|TRUE|FALSE|T|F|Inf|NaN)\\b",e:hljs.IMMEDIATE_RE,r:1},{cN:"identifier",b:"[a-zA-Z.][a-zA-Z0-9._]*\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"operator",b:"<\\-(?!\\s*\\d)",e:hljs.IMMEDIATE_RE,r:2},{cN:"operator",b:"\\->|<\\-",e:hljs.IMMEDIATE_RE,r:1},{cN:"operator",b:"%%|~",e:hljs.IMMEDIATE_RE},{cN:"operator",b:">=|<=|==|!=|\\|\\||&&|=|\\+|\\-|\\*|/|\\^|>|<|!|&|\\||\\$|:",e:hljs.IMMEDIATE_RE,r:0},{cN:"operator",b:"%",e:"%",i:"\\n",r:1},{cN:"identifier",b:"`",e:"`",r:0},{cN:"string",b:'"',e:'"',c:[hljs.BE],r:0},{cN:"string",b:"'",e:"'",c:[hljs.BE],r:0},{cN:"paren",b:"[[({\\])}]",e:hljs.IMMEDIATE_RE,r:0}]}};
    hljs.initHighlightingOnLoad();
    </script>

    <!-- MathJax scripts -->
    <script type="text/javascript" src="js/MathJax/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    MathJax.Hub.Config({    
      extensions: ["tex2jax.js"],    
      "HTML-CSS": { scale: 100}    
    });    
    </script>
    <link href="bootstrap/css/bootstrap-responsive.css" rel="stylesheet">

    <!-- HTML5 shim, for IE6-8 support of HTML5 elements -->
    <!--[if lt IE 9]>
      <script src="js/html5shiv.js"></script>
    <![endif]-->

    <!-- Fav and touch icons -->
    <link rel="apple-touch-icon-precomposed" sizes="144x144" href="ico/apple-touch-icon-144-precomposed.png">
    <link rel="apple-touch-icon-precomposed" sizes="114x114" href="ico/apple-touch-icon-114-precomposed.png">
      <link rel="apple-touch-icon-precomposed" sizes="72x72" href="ico/apple-touch-icon-72-precomposed.png">
                    <link rel="apple-touch-icon-precomposed" href="ico/apple-touch-icon-57-precomposed.png">
                                   <!-- <link rel="shortcut icon" href="ico/favicon.png"> -->
  </head>

  <body>

    <div class="container-narrow">

      <div class="masthead">
        <ul class="nav nav-pills pull-right">
          <li class="active"><a href="index.html">Docs</a></li>
          <li class=""><a href="functionref.html">Function Ref</a></li>
          <li><a href="https://www.github.com/hafen/datadr">Github</a></li>
        </ul>
        <p class="myHeader">datadr: Divide and Recombine in R</p>
      </div>

      <hr>

<div class="container-fluid">
   <div class="row-fluid">
   
   <div class="span3 well">
   <ul class = "nav nav-list" id="toc">
   <li class='nav-header unselectable' data-edit-href='https://www.github.com/hafen/datadr/edit/gh-pages/docs/1intro.Rmd'>Intro</li>
      
      <li class='active'>
         <a target='_self' class='nav-not-header' href='#background'>Background</a>
      </li>


      <li class='active'>
         <a target='_self' class='nav-not-header' href='#getting-started'>Getting Started</a>
      </li>


<li class='nav-header unselectable' data-edit-href='https://www.github.com/hafen/datadr/edit/gh-pages/docs/2data.Rmd'>Dealing with Data in D&R</li>
      
      <li class='active'>
         <a target='_self' class='nav-not-header' href='#key-value-pairs'>Key-Value Pairs</a>
      </li>


      <li class='active'>
         <a target='_self' class='nav-not-header' href='#distributed-data-objects'>Distributed Data Objects</a>
      </li>


      <li class='active'>
         <a target='_self' class='nav-not-header' href='#distributed-data-frames'>Distributed Data Frames</a>
      </li>


<li class='nav-header unselectable' data-edit-href='https://www.github.com/hafen/datadr/edit/gh-pages/docs/3dnr.Rmd'>Division and Recombination</li>
      
      <li class='active'>
         <a target='_self' class='nav-not-header' href='#high-level-interface'>High-Level Interface</a>
      </li>


      <li class='active'>
         <a target='_self' class='nav-not-header' href='#division'>Division</a>
      </li>


      <li class='active'>
         <a target='_self' class='nav-not-header' href='#recombination'>Recombination</a>
      </li>


      <li class='active'>
         <a target='_self' class='nav-not-header' href='#between-subset-variables'>Between-Subset Variables</a>
      </li>


      <li class='active'>
         <a target='_self' class='nav-not-header' href='#dr-examples'>D&R Examples</a>
      </li>


<li class='nav-header unselectable' data-edit-href='https://www.github.com/hafen/datadr/edit/gh-pages/docs/4mr.Rmd'>MapReduce</li>
      
      <li class='active'>
         <a target='_self' class='nav-not-header' href='#introduction-to-mapreduce'>Introduction to MapReduce</a>
      </li>


      <li class='active'>
         <a target='_self' class='nav-not-header' href='#mapreduce-with-datadr'>MapReduce with datadr</a>
      </li>


      <li class='active'>
         <a target='_self' class='nav-not-header' href='#mapreduce-examples'>MapReduce Examples</a>
      </li>


      <li class='active'>
         <a target='_self' class='nav-not-header' href='#other-options'>Other Options</a>
      </li>


<li class='nav-header unselectable' data-edit-href='https://www.github.com/hafen/datadr/edit/gh-pages/docs/5divag.Rmd'>Division-Agnostic Methods</li>
      
      <li class='active'>
         <a target='_self' class='nav-not-header' href='#all-data-computation'>All-Data Computation</a>
      </li>


      <li class='active'>
         <a target='_self' class='nav-not-header' href='#quantiles'>Quantiles</a>
      </li>


<li class='nav-header unselectable' data-edit-href='https://www.github.com/hafen/datadr/edit/gh-pages/docs/6backend.Rmd'>Store/Compute Backends</li>
      
      <li class='active'>
         <a target='_self' class='nav-not-header' href='#backend-choices'>Backend Choices</a>
      </li>


      <li class='active'>
         <a target='_self' class='nav-not-header' href='#small-memory--cpu'>Small: Memory / CPU</a>
      </li>


      <li class='active'>
         <a target='_self' class='nav-not-header' href='#medium-disk--multicore'>Medium: Disk / Multicore</a>
      </li>


      <li class='active'>
         <a target='_self' class='nav-not-header' href='#large-hdfs--rhipe'>Large: HDFS / RHIPE</a>
      </li>


      <li class='active'>
         <a target='_self' class='nav-not-header' href='#conversion'>Conversion</a>
      </li>


<li class='nav-header unselectable' data-edit-href='https://www.github.com/hafen/datadr/edit/gh-pages/docs/7faq.Rmd'>Misc</li>
      
      <li class='active'>
         <a target='_self' class='nav-not-header' href='#debugging'>Debugging</a>
      </li>


      <li class='active'>
         <a target='_self' class='nav-not-header' href='#faq'>FAQ</a>
      </li>


      <li class='active'>
         <a target='_self' class='nav-not-header' href='#r-code'>R Code</a>
      </li>

   </ul>
   </div>

<div class="span9 tab-content" id="main-content">

<div class='tab-pane active' id='background'>
<h3>Background</h3>

<p>This tutorial covers an implementation of Divide and Recombine (D&amp;R) in the R statistical programming environment, called <code>datadr</code>.</p>

<p>The goal of D&amp;R is to provide an environment for data analysts to carry out deep statistical analysis of large, complex data with as much ease and flexibility as is possible with small datasets.  </p>

<p>D&amp;R is accomplished by dividing data into meaningful subsets, applying analytical methods to those subsets, and recombining the results.  Recombinations can be numerical or visual.  For visualization in the D&amp;R framework, see <a href="http://github.com/hafen/trelliscope">trelliscope</a>.  </p>

<p>The diagram below is a visual representation of the D&amp;R process.</p>

<p><img src="image/drdiagram.svg" width="650px" alt="drdiagram" style="display:block; margin:auto"/></p>

<!-- ![drdiagram](image/drdiagram.png) -->

<p>The raw data is stored in some arbitrary structure.  We apply a division method to it to obtain a meaningful partitioning.  Then we attack this partitioning with several visual and numerical recombination methods, where we apply the method independently to each subset and combine the results.  There are many forms of divisions and recombinations, many of which will be covered in this tutorial.</p>

<!-- In this approach, results of several numerical routines might not match an exact all-data result, but we seek division and recombination schemes that provide approximations that are.   -->

<p>A clearer picture of how D&amp;R works should be reached by reading and trying out the examples in the documentation.  It is also recommended to read the references below.</p>

<h4>Outline</h4>

<ul>
<li>First, we cover the foundational D&amp;R data structure, key-value pairs, and how they are used to build distributed data objects and distributed data frames.</li>
<li>Next, we provide an introduction to the high-level division and recombination methods in <code>datadr</code>.</li>
<li>Then we discuss MapReduce -- the lower-level language for accomplishing D&amp;R tasks -- which is the engine for the higher-level D&amp;R methods.  It is anticipated that the high-level language will be sufficient for most analysis tasks, but the lower-level approach is also exposed for special cases.<br></li>
<li>We then cover some division-agnostic methods that do various computations across the entire data set, regardless of how it is divided, such as all-data quantiles.<br></li>
<li>For all of these discussions, we use small data sets that fit in memory for illustrative purposes.  This way everyone can follow along without having a large-scale backend like Hadoop running and configured.  However, the true power of D&amp;R is with large data sets, and after introducing all of this material, we cover different backends for computation and storage that are currently supported for D&amp;R.  The interface always remains the same regardless of the backend, but there are various things to discuss for each case.  The backends discussed are:

<ul>
<li><strong>in-memory / single core R:</strong> ideal for small data</li>
<li><strong>local disk / multicore R:</strong> ideal for medium-sized data (too big for memory, small enough for local disk)</li>
<li><strong>Hadoop Distributed File System (HDFS) / RHIPE / Hadoop MapReduce:</strong> ideal for very large data sets</li>
</ul></li>
<li>We also provide R source files for all of the examples throughout the documentation.</li>
</ul>

<div class="alert alert-warning"><strong>Note:</strong> Throughout the tutorial, the examples cover very small, simple datasets.  This is by design, as the focus is on getting familiar with the available commands.  Keep in mind that the same interface works for very large datasets, and that design choices have been made with scalability in mind.</div>

<h4>Reference</h4>

<p>Related projects:</p>

<ul>
<li><a href="http://github.com/saptarshiguha/RHIPE">RHIPE</a>: the engine that makes D&amp;R work for large datasets</li>
<li><a href="http://github.com/hafen/trelliscope">trelliscope</a>: the visualization companion to <code>datadr</code></li>
</ul>

<p>References:</p>

<ul>
<li><a href="http://datadr.org">datadr.org</a></li>
<li><a href="http://onlinelibrary.wiley.com/doi/10.1002/sta4.7/full">Large complex data: divide and recombine (D&amp;R) with RHIPE. <em>Stat</em>, 1(1), 53-67</a></li>
</ul>

</div>


<div class='tab-pane' id='getting-started'>
<h3>Getting Started</h3>

<p>It is easy to get up and running with <code>datadr</code>.  One needs to have the <code>devtools</code> package installed (available on CRAN), after which the <code>datadr</code> package can simply be installed with the following:</p>

<pre><code class="r">library(devtools)
install_github(&quot;datadr&quot;, &quot;hafen&quot;)
</code></pre>

<p>Then we load the package:</p>

<pre><code class="r">library(datadr)
</code></pre>

<p>and we are ready to go.</p>

<h4>RHIPE</h4>

<p>Simply installing <code>datadr</code> on a local workstation is sufficient for trying out the framework with small and medium-sized data sets.  For very large data sets, however, the RHIPE backend is necessary.  </p>

<p>RHIPE is the R and Hadoop Integrated Programming Environment.  It provides a way to execute Hadoop MapReduce jobs completely from within R and with R data structures.</p>

<p>To install and use RHIPE, the following are required:</p>

<ol>
<li>A cluster of machines (a single node can be used but it pointless outside of testing) -- these machines can be commodity workstations</li>
<li>Hadoop installed and configured on the cluster</li>
<li>RHIPE and its dependencies (protocol buffers) installed on all the nodes</li>
</ol>

<p>(1) is often a large barrier to entry.  (2) can require a lot of patience and know-how.  (3) isn&#39;t too difficult.  </p>

<p>These requirements are generally enough of a hinderance that only people very serious about scalable data analysis have the perseverance to get a system running.  Unfortunately, this is currently the price to pay for scalability.  We are working on providing easier access and better documentation for getting set up with this computing platform.</p>

</div>


<div class='tab-pane' id='key-value-pairs'>
<h3>Key-Value Pairs</h3>

<p>The most basic data structure in D&amp;R is a <em>key-value pair</em> which is simply a data structure with key and value elements, each of which can have any data structure.  </p>

<h4>Key-value pairs in <code>datadr</code></h4>

<p>In <code>datadr</code>, key-value pairs are R lists with two elements, one for the key and one for the value.  For example,</p>

<pre><code class="r"># simple key-value pair example
list(1:5, rnorm(10))
</code></pre>

<pre><code>[[1]]
[1] 1 2 3 4 5

[[2]]
 [1] -1.2071  0.2774  1.0844 -2.3457  0.4291  0.5061 -0.5747 -0.5466
 [9] -0.5645 -0.8900
</code></pre>

<p>is a key-value pair with integers 1-5 as the key and 10 random normals as the value.  Typically, a key is used as a unique identifier for the value.  For <code>datadr</code> it is recommended to make the key a simple string when possible.</p>

<h4>Key-value pair collections</h4>

<p>D&amp;R data objects are made up of collections of key-value pairs.  In <code>datadr</code>, these are represented as lists of key-value pair lists.  As an example, consider the <code>iris</code> data set, which consists of measurements of 4 aspects for 50 flowers from each of 3 species of iris.  Suppose we would like to split the sepal measurements of the iris data into key-value pairs by species:</p>

<pre><code class="r"># create by-species key-value pairs
irisKV &lt;- list(
   list(&quot;setosa&quot;, subset(iris, Species==&quot;setosa&quot;)[,1:2]),
   list(&quot;versicolor&quot;, subset(iris, Species==&quot;versicolor&quot;)[,1:2]),
   list(&quot;virginica&quot;, subset(iris, Species==&quot;virginica&quot;)[,1:2])
)
irisKV
</code></pre>

<pre><code>[[1]]
[[1]][[1]]
[1] &quot;setosa&quot;

[[1]][[2]]
   Sepal.Length Sepal.Width
1           5.1         3.5
2           4.9         3.0
3           4.7         3.2
4           4.6         3.1
5           5.0         3.6
 [ reached getOption(&quot;max.print&quot;) -- omitted 45 rows ]


[[2]]
[[2]][[1]]
[1] &quot;versicolor&quot;

[[2]][[2]]
    Sepal.Length Sepal.Width
51           7.0         3.2
52           6.4         3.2
53           6.9         3.1
54           5.5         2.3
55           6.5         2.8
 [ reached getOption(&quot;max.print&quot;) -- omitted 45 rows ]


[[3]]
[[3]][[1]]
[1] &quot;virginica&quot;

[[3]][[2]]
    Sepal.Length Sepal.Width
101          6.3         3.3
102          5.8         2.7
103          7.1         3.0
104          6.3         2.9
105          6.5         3.0
 [ reached getOption(&quot;max.print&quot;) -- omitted 45 rows ]
</code></pre>

<p>The result is a list of 3 key-value pairs.  We chose the species to be the key and the corresponding data frame of sepal measurements to be the value for each pair.</p>

<p>This example shows how we can partition our data into key-value pairs that have meaning -- each subset represents measurements for one species.  The ability to divide the data up into pieces allows us to distribute datasets that might be too large for a single disk across multiple machines, and also allows us to distribute computation, because in recombination we apply methods independently to each subset.</p>

<p>Here, we manually created the partition by species, but <code>datadr</code> provides simple mechanisms for specifying divisions, which we will cover <a href="#division">later in the tutorial</a>.  Prior to doing that, however, we need to discuss how collections of key-value pairs are represented in <code>datadr</code> as distributed data objects.</p>

<h4>Applying functions to key-value pairs</h4>

<p>As a final note on key-value pairs before moving on to <a href="#distributed-data-objects">distributed data objects</a>, this is a good place to discuss how functions are applied to key-value pairs in <code>datadr</code>.  There are many places in <code>datadr</code> methods where you can specify functions that you want to have applied to each key-value pair for various purposes.  These include:</p>

<ul>
<li><code>transFn</code>: an argument to <code>ddo()</code></li>
<li><code>preTransFn</code>, <code>postTransFn</code>, <code>bsvFn</code>, and <code>filterFn</code>: arguments to <code>divide()</code></li>
<li><code>apply</code>: an argument to <code>recombine()</code></li>
<li><code>panelFn</code> and <code>cogFn</code>: arguments to <code>makeDisplay()</code> in the <code>trelliscope</code> package</li>
</ul>

<p>There is a general approach for flexibly specifying functions that operate on key-value pairs in <code>datadr</code>.  In some cases, you may want both the key and value to be available in the function for your code to operate on, but many times all you care about is applying the function to the value.  </p>

<p>To keep the writing of such functions simple, in <code>datadr</code>, a check is made to see how many formal arguments your function has.  If it has one argument, that argument is treated as the value.  If it has two arguments, the first is treated as the key and the second as the value.  Handling of this logic is done by a function <code>kvApply(fn, kvPair)</code>.</p>

<p>For example, suppose I want to apply a function computing the mean sepal length to the first key-value pair of the <code>irisKV</code> data:</p>

<pre><code class="r"># kvApply example operating on just value
meanSepalLength1 &lt;- function(v)
   mean(v$Sepal.Length)

kvApply(meanSepalLength1, irisKV[[1]])
</code></pre>

<pre><code>[1] 5.006
</code></pre>

<p>Here my function <code>meanSepalLength1</code> only takes one argument so it is passed the value, which is the data frame of sepal measurements for this subset.  </p>

<p>However, suppose there is information in the key that I would like to include in my computation or output.  For example, suppose I want a data frame of the mean tagged by its associated species (the key):</p>

<pre><code class="r"># kvApply example operating on key and value
meanSepalLength2 &lt;- function(k, v)
   data.frame(species=k, mean=mean(v$Sepal.Length))

kvApply(meanSepalLength2, irisKV[[1]])
</code></pre>

<pre><code>  species  mean
1  setosa 5.006
</code></pre>

<p>My function <code>meanSepalLength2</code> now takes two arguments, and therefore <code>kvApply</code> will provide it both the key and the value, available in the function as <code>k</code> and <code>v</code>, and the function can use the key to get the species to put in the resulting data frame.</p>

<div class="alert alert-warning">
Keep in mind that this is how things are done for all supplied functions to arguments of <code>datadr</code> methods that operate on key-value pairs: functions with two arguments are passed the key and value, and functions with one argument are passed only the value.
</div>

<!-- The reason for this construct is that in our experience we want to apply most functions to just the value, but there always arise cases where we want information in the key as well.  We could simply have all functions expect one argument which is the key-value pair list, but then there is too much subsetting to get keys and values that is tedious and makes code less readable. -->

</div>


<div class='tab-pane' id='distributed-data-objects'>
<h3>Distributed Data Objects</h3>

<p>In <code>datadr</code>, a collection of key-value pairs along with attributes about the collection constitute a distributed data object (ddo).  Most <code>datadr</code> operations require &quot;ddo&quot; objects, and hence it is important to represent key-value pair collections as such.</p>

<h4>Initializing a &quot;ddo&quot; object</h4>

<p>To initialize a collection of key-value pairs as a distributed data object, we use the <code>ddo()</code> function:</p>

<pre><code class="r"># create ddo object from irisKV
irisDdo &lt;- ddo(irisKV)
</code></pre>

<pre><code>* Getting basic &#39;ddo&#39; attributes...
</code></pre>

<p><code>ddo()</code> simply takes the collection of key-value pairs and attaches additional attributes to the resulting &quot;ddo&quot; object.  Note that in this example, since the data is in memory, we are supplying the data directly as the argument to <code>ddo()</code>.  For larger datasets stored in more scalable backends, instead of passing the data directly, a connection that points to where the key-value pairs are stored is provided.  This is discussed in more detail in the <a href="#backend-choices">Store/Compute Backends</a> sections.</p>

<p>Objects of class &quot;ddo&quot; have several methods that can be invoked on them.  The most simple of these is a print method:</p>

<pre><code class="r">irisDdo
</code></pre>

<pre><code>
Distributed data object of class &#39;kvMemory&#39; with attributes: 

&#39;ddo&#39; attribute | value
----------------+-----------------------------------------------------------
 keys           | keys are available through getKeys(dat)
 totSize        | 5.57 KB
 nDiv           | 3
 splitSizeDistn | [empty] call updateAttributes(dat) to get this value
 example        | use kvExample(dat) to get an example subset
 bsvInfo        | [empty] no BSVs have been specified

In-memory data connection
</code></pre>

<p>The print method shows several attributes that have been computed for the data.</p>

<h4>&quot;ddo&quot; attributes</h4>

<p>From the printout of <code>irisDdo</code>, we see that a &quot;ddo&quot; object has several attributes.  The most basic ones:</p>

<ul>
<li><code>totSize</code>: The total size of the data is 6 KB (that&#39;s some big data!)</li>
<li><code>nDiv</code>: There are 3 divisions</li>
</ul>

<p>We can look at the keys with</p>

<pre><code class="r"># look at irisDdo keys
getKeys(irisDdo)
</code></pre>

<pre><code>$`946a2c38121bed59091a362f5015327e`
[1] &quot;setosa&quot;

$fa66f5fefadcc79a57a5afe78fe680db
[1] &quot;versicolor&quot;

$b313f00809c319b9b5918795d13ca47a
[1] &quot;virginica&quot;
</code></pre>

<p>We can also get an example key-value pair with</p>

<pre><code class="r"># look at an example key-value pair of irisDdo
kvExample(irisDdo)
</code></pre>

<pre><code>[[1]]
[1] &quot;setosa&quot;

[[2]]
   Sepal.Length Sepal.Width
1           5.1         3.5
2           4.9         3.0
3           4.7         3.2
4           4.6         3.1
5           5.0         3.6
 [ reached getOption(&quot;max.print&quot;) -- omitted 45 rows ]
</code></pre>

<p><code>kvExample</code> is useful for obtaining a subset key-value pair against which we can test out different analytical methods before applying them across the entire data set.</p>

<p>The other attributes, <code>splitSizeDistn</code> and <code>bsvInfo</code> are empty.  <code>bsvInfo</code> provides information about between subset variables (BSVs), which we will discuss <a href="#between-subset-variables">later</a>.  </p>

<p>The <code>splitSizeDistn</code> attribute provides information about the quantiles of the distribution of the size of each division.  With very large data sets with a large number of subsets, this can be useful for getting a feel for how uniform the subset sizes are.  </p>

<p>The <code>splitSizeDistn</code> attribute and more that we will see in the future are not computed by default when <code>ddo()</code> is called.  This is because it requires a computation over the data set, which can take some time with very large datasets, and may not always be desired or necessary.</p>

<h4>Updating attributes</h4>

<p>If you decide at any point that you would like to update the attributes of your &quot;ddo&quot; object, you can call:</p>

<pre><code class="r"># update irisDdo attributes
irisDdo &lt;- updateAttributes(irisDdo)
</code></pre>

<pre><code>* Running map/reduce to get missing attributes...
* Getting basic &#39;ddo&#39; attributes...
</code></pre>

<pre><code class="r">irisDdo
</code></pre>

<pre><code>
Distributed data object of class &#39;kvMemory&#39; with attributes: 

&#39;ddo&#39; attribute | value
----------------+-----------------------------------------------------------
 keys           | keys are available through getKeys(dat)
 totSize        | 5.57 KB
 nDiv           | 3
 splitSizeDistn | use splitSizeDistn(dat) to get distribution
 example        | use kvExample(dat) to get an example subset
 bsvInfo        | [empty] no BSVs have been specified

In-memory data connection
</code></pre>

<p>The <code>splitSizeDistn</code> attribute is now available.  We can look at it with the accessor <code>splitSizeDistn()</code>:</p>

<pre><code class="r"># plot distribution of the size of the key-value pairs
plot(splitSizeDistn(irisDdo))
</code></pre>

<p><img src="figure/plot_iris_split_size.png" alt="plot of chunk plot_iris_split_size"> </p>

<p>Another way to get updated attributes is at the time the &quot;ddo&quot; object is created, by setting <code>update=TRUE</code>:</p>

<pre><code class="r"># update at the time ddo() is called
irisDdo &lt;- ddo(irisKV, update=TRUE)
</code></pre>

<pre><code>* Getting basic &#39;ddo&#39; attributes...
* Running map/reduce to get missing attributes...
* Getting basic &#39;ddo&#39; attributes...
</code></pre>

<h4>Note about storage and computation</h4>

<p>Notice the first and final lines of output from the <code>irisDdo</code> object printout.  It states that the object is of class &quot;kvMemory&quot; (key-value pairs in memory), and that it has an &quot;in-memory data connection&quot;.  Here we are playing with a very small data set, but this package is designed to scale.  </p>

<p>We will talk about other backends for storing and processing larger data sets that don&#39;t fit in memory or even on your workstation&#39;s disk.  The key here is that the interface always stays the same, regardless of whether we are working with terabytes of kilobytes of data.</p>

<h4>Accessing subsets</h4>

<p>We can access subsets of the data by key or by index:</p>

<pre><code class="r">irisDdo[[&quot;setosa&quot;]]
</code></pre>

<pre><code>[[1]]
[1] &quot;setosa&quot;

[[2]]
   Sepal.Length Sepal.Width
1           5.1         3.5
2           4.9         3.0
3           4.7         3.2
4           4.6         3.1
5           5.0         3.6
 [ reached getOption(&quot;max.print&quot;) -- omitted 45 rows ]
</code></pre>

<pre><code class="r">irisDdo[[1]]
</code></pre>

<pre><code>[[1]]
[1] &quot;setosa&quot;

[[2]]
   Sepal.Length Sepal.Width
1           5.1         3.5
2           4.9         3.0
3           4.7         3.2
4           4.6         3.1
5           5.0         3.6
 [ reached getOption(&quot;max.print&quot;) -- omitted 45 rows ]
</code></pre>

<pre><code class="r">irisDdo[c(&quot;setosa&quot;, &quot;virginica&quot;)]
</code></pre>

<pre><code>[[1]]
[[1]][[1]]
[1] &quot;setosa&quot;

[[1]][[2]]
   Sepal.Length Sepal.Width
1           5.1         3.5
2           4.9         3.0
3           4.7         3.2
4           4.6         3.1
5           5.0         3.6
 [ reached getOption(&quot;max.print&quot;) -- omitted 45 rows ]


[[2]]
[[2]][[1]]
[1] &quot;virginica&quot;

[[2]][[2]]
    Sepal.Length Sepal.Width
101          6.3         3.3
102          5.8         2.7
103          7.1         3.0
104          6.3         2.9
105          6.5         3.0
 [ reached getOption(&quot;max.print&quot;) -- omitted 45 rows ]
</code></pre>

<pre><code class="r">irisDdo[1:2]
</code></pre>

<pre><code>[[1]]
[[1]][[1]]
[1] &quot;setosa&quot;

[[1]][[2]]
   Sepal.Length Sepal.Width
1           5.1         3.5
2           4.9         3.0
3           4.7         3.2
4           4.6         3.1
5           5.0         3.6
 [ reached getOption(&quot;max.print&quot;) -- omitted 45 rows ]


[[2]]
[[2]][[1]]
[1] &quot;versicolor&quot;

[[2]][[2]]
    Sepal.Length Sepal.Width
51           7.0         3.2
52           6.4         3.2
53           6.9         3.1
54           5.5         2.3
55           6.5         2.8
 [ reached getOption(&quot;max.print&quot;) -- omitted 45 rows ]
</code></pre>

<p>Accessing by key is much simpler when the key is a character string, but subsetting works even when passing a list of non-string keys.</p>

</div>


<div class='tab-pane' id='distributed-data-frames'>
<h3>Distributed Data Frames</h3>

<p>Key-value pairs in distributed data objects can have any structure.  If we constrain the values to be data frames or readily transformable into data frames, we can represent the object as a distributed data frame (ddf).  Having a uniform data frame structure for the values provides several benefits and data frames are required for specifying division methods.</p>

<h4>Initializing a &quot;ddf&quot; object</h4>

<p>Our <code>irisKV</code> data we created earlier has values that are data frames, so we can cast it as a distributed data frame like this:</p>

<pre><code class="r"># create ddf object from irisKV
irisDdf &lt;- ddf(irisKV, update=TRUE)
</code></pre>

<pre><code>* Getting basic &#39;ddo&#39; attributes...
* Getting basic &#39;ddf&#39; attributes...
* Running map/reduce to get missing attributes...
* Getting basic &#39;ddo&#39; attributes...
</code></pre>

<pre><code class="r">irisDdf
</code></pre>

<pre><code>
Distributed data object of class &#39;kvMemory&#39; with attributes: 

&#39;ddo&#39; attribute | value
----------------+-----------------------------------------------------------
 keys           | keys are available through getKeys(dat)
 totSize        | 5.57 KB
 nDiv           | 3
 splitSizeDistn | use splitSizeDistn(dat) to get distribution
 example        | use kvExample(dat) to get an example subset
 bsvInfo        | [empty] no BSVs have been specified

&#39;ddf&#39; attribute | value
----------------+-----------------------------------------------------------
 vars           | Sepal.Length(num), Sepal.Width(num)
 transFn        | identity (original data is a data frame)
 nRow           | 150
 splitRowDistn  | use splitRowDistn(dat) to get distribution
 summary        | use summary(dat) to see summaries

In-memory data connection
</code></pre>

<h4>&quot;ddf&quot; attributes</h4>

<p>The printout of <code>irisDdf</code> above shows data-frame-related attributes (which were automatically updated because we specified <code>update=TRUE</code>) in addition to the &quot;ddo&quot; attributes we saw earlier.  These include:</p>

<ul>
<li><code>vars</code>: a list of the variables</li>
<li><code>transFn</code>: a transformation function (more on this later)</li>
<li><code>nrow</code>: the total number of rows in the data set</li>
<li><code>splitRowDistn</code>: which is similar to <code>splitSizeDistn</code>, except that it is the distribution of the number of rows of data in each subset</li>
<li><code>summary</code> attribute holds summary statistics about each variable in the data frame</li>
</ul>

<p>The <code>summary</code> attribute can be useful for later computations, where doing it once up front is more efficient than .  A good example is quantile estimation (see <code>?quantile.ddo</code>), where the range is required to get a good quantile approximation.  Summary statistics are all computed simultaneously in one MapReduce job with a call to <code>updateAttributes()</code>.  </p>

<p>The numerical summary statistics are computed using a numerically stable algorithm (cite).  Summary statistics include:</p>

<p>For each numeric variable: </p>

<ul>
<li><code>nna</code>: number of missing values</li>
<li><code>stats</code>: list of mean, variance, skewness, kurtosis</li>
<li><code>range</code>: min, max</li>
</ul>

<p>For each categorical variable:</p>

<ul>
<li><code>nobs</code>: number of observations</li>
<li><code>nna</code>: number of missing values</li>
<li><code>freqTable</code>: a data frame containing a frequency table</li>
</ul>

<p>Summaries can be accessed by:</p>

<pre><code class="r"># look at irisDdf summary stats
summary(irisDdf)
</code></pre>

<pre><code>$Sepal.Length
$Sepal.Length$nna
[1] 0

$Sepal.Length$stats
$Sepal.Length$stats$mean
[1] 5.843

$Sepal.Length$stats$var
[1] 0.6857

$Sepal.Length$stats$skewness
[1] 0.3118

$Sepal.Length$stats$kurtosis
[1] 2.426


$Sepal.Length$range
[1] 4.3 7.9


$Sepal.Width
$Sepal.Width$nna
[1] 0

$Sepal.Width$stats
$Sepal.Width$stats$mean
[1] 3.057

$Sepal.Width$stats$var
[1] 0.19

$Sepal.Width$stats$skewness
[1] 0.3158

$Sepal.Width$stats$kurtosis
[1] 3.181


$Sepal.Width$range
[1] 2.0 4.4


attr(,&quot;class&quot;)
[1] &quot;ddfSummary&quot; &quot;list&quot;      
</code></pre>

<p>In the future, there will be a better print method, etc. for summary results.</p>

<h4>Data frame-like &quot;ddf&quot; methods</h4>

<p>Note that with an object of class &quot;ddf&quot;, you can use some of the methods that apply to regular data frames:</p>

<pre><code class="r">nrow(irisDdf)
</code></pre>

<pre><code>[1] 150
</code></pre>

<pre><code class="r">ncol(irisDdf)
</code></pre>

<pre><code>[1] 2
</code></pre>

<pre><code class="r">names(irisDdf)
</code></pre>

<pre><code>[1] &quot;Sepal.Length&quot; &quot;Sepal.Width&quot; 
</code></pre>

<h4>Passing a data frame to <code>ddo()</code> and <code>ddf()</code></h4>

<p>It is worth noting that it is possible to pass a single data frame to <code>ddo()</code> or <code>ddf()</code>.  The result is a single key-value pair with the data frame as the value, and &quot;&quot; as the key.  This is an option strictly for convenience and with the idea that further down the line operations will be applied that split the data up into a more useful set of key-value pairs.  Here is an example:</p>

<pre><code class="r"># initialize ddf from a data frame
irisDf &lt;- ddf(iris, update=TRUE)
</code></pre>

<pre><code>* Getting basic &#39;ddo&#39; attributes...
* Getting basic &#39;ddf&#39; attributes...
* Running map/reduce to get missing attributes...
* Getting basic &#39;ddo&#39; attributes...
</code></pre>

<p>This of course only makes sense for data small enough to fit in memory in the first place.  In the <a href="#small-memory--cpu">backends</a> section, we will discuss other backends for larger data and how data can be added to objects in these cases.</p>

<h4>Coercing to &quot;ddf&quot; with a Transformation Function</h4>

<p>There may be times where we want to create a ddf but we have data with values that either aren&#39;t stored as a data frame or we don&#39;t want to store as a data frame, but we want them to act like data frames.  In this case, there is an argument to <code>ddf()</code> called <code>transFn</code> which transforms the data into a data frame prior to any computation is carried out on it.</p>

<p>Consider the following collection of key-value pairs, for example:</p>

<pre><code class="r"># example of some &quot;less-structured&quot; key-value pairs
people &lt;- list(
   list(&quot;fred&quot;, 
      list(age=74, statesLived=c(&quot;NJ&quot;, &quot;MA&quot;, &quot;ND&quot;, &quot;TX&quot;))
   ),
   list(&quot;bob&quot;, 
      list(age=42, statesLived=&quot;NJ&quot;)
   )
)
</code></pre>

<p>The values are lists, not data frames.  In this case, the values are easily coercible to data frames using <code>as.data.frame</code>:</p>

<pre><code class="r"># cast first value as data frame
as.data.frame(people[[1]][[2]])
</code></pre>

<pre><code>  age statesLived
1  74          NJ
2  74          MA
3  74          ND
4  74          TX
</code></pre>

<p>Since <code>people</code> is a list of key-value pairs, we extract the first pair with <code>[[1]]</code>, and then get the value of the pair with <code>[[2]]</code>.</p>

<p>We see that <code>as.data.frame</code> is able to coerce the list to a data frame.  So if we want to treat <code>people</code> as a distributed data frame, we might simply provide <code>as.data.frame</code> as the transformation function:</p>

<pre><code class="r"># ddf with transFn
peopleDdf &lt;- ddf(people, transFn=as.data.frame)
</code></pre>

<pre><code>* Getting basic &#39;ddo&#39; attributes...
* Getting basic &#39;ddf&#39; attributes...
</code></pre>

<p><code>ddf()</code> actually tries <code>as.data.frame</code> if the default (<code>identity</code>) does not yield a data frame:</p>

<pre><code class="r"># ddf tries as.data.frame for transFn by default
peopleDdf &lt;- ddf(people)
</code></pre>

<pre><code>* Getting basic &#39;ddo&#39; attributes...
</code></pre>

<pre><code>Warning: Data is not strictly a data frame, but coercible using
as.data.frame
</code></pre>

<pre><code>* Getting basic &#39;ddf&#39; attributes...
</code></pre>

<p>With this data representation, is what things will look like to <code>datadr</code> methods that need the data to be a data frame:</p>

<pre><code class="r"># get a ddf key-value pair with transFn applied
kvExample(peopleDdf, transform=TRUE)
</code></pre>

<pre><code>[[1]]
[1] &quot;fred&quot;

[[2]]
  age statesLived
1  74          NJ
2  74          MA
3  74          ND
4  74          TX
</code></pre>

<!-- The `transform=TRUE` argument tells `kvExample()` to apply `transFn` to the result. -->

<p>But how the data actually is stored is still the same:</p>

<pre><code class="r"># data is still stored unstructured (pre transFn)
kvExample(peopleDdf)
</code></pre>

<pre><code>[[1]]
[1] &quot;fred&quot;

[[2]]
[[2]]$age
[1] 74

[[2]]$statesLived
[1] &quot;NJ&quot; &quot;MA&quot; &quot;ND&quot; &quot;TX&quot;
</code></pre>

<p>More complex examples will certainly arise where a non-trivial transformation function is required to cast the data as a data frame.  </p>

<p>This transformation is honored in <code>datadr</code> methods such as <code>divide()</code> and <code>updateAttributes()</code>.</p>

</div>


<div class='tab-pane' id='high-level-interface'>
<h3>High-Level Interface</h3>

<p><code>datadr</code> provides a high-level language for D&amp;R that simply consists of functions <code>divide()</code> for performing division, and <code>recombine()</code> for performing recombinations.  The goal is for these methods to be sufficient for most operations a user might want to carry out.  There are several ways these methods can be invoked to perform different tasks, which is outlined in this section.  </p>

<p>At their simplest form, <code>divide()</code> and <code>recombine()</code> provide a way to create a persistent partitioning of the data and then perform a <code>lapply</code>-like operation over the data with different ways to combine the results.  Being able to easily perform these operations alone provides a lot of power for ad-hoc analysis of very large data sets.  However, results from D&amp;R theory and methods continue to be injected into these methods to provide an even more rich environment for analysis.</p>

<p><img src="image/drdiagram.svg" width="650px" alt="drdiagram" style="display:block; margin:auto"/></p>

<!-- ![drdiagram](image/drdiagram.png) -->

</div>


<div class='tab-pane' id='division'>
<h3>Division</h3>

<p>Division is achieved through the <code>divide()</code> method.  The function documentation is available <a href="http://hafen.github.io/datadr/functionref.html#recombine">here</a>.</p>

<p>Currently there are two types of divisions supported: <em>conditioning variable</em>, and <em>random replicate</em>.  In this section we discuss the major arguments to <code>divide()</code>, the most important of which is <code>by</code>.</p>

<h4>The <code>by</code> argument: <em>conditioning variable division</em></h4>

<p>In the previous section, we were looking at a division of the <code>iris</code> data by species.  We manually split the data into key-value pairs.  We can achieve the same result by doing conditioning variable division:</p>

<pre><code class="r">irisDdf &lt;- ddf(iris)
</code></pre>

<pre><code>* Getting basic &#39;ddo&#39; attributes...
* Getting basic &#39;ddf&#39; attributes...
</code></pre>

<pre><code class="r"># divide irisDdf by species
bySpecies &lt;- divide(irisDdf, by=&quot;Species&quot;, update=TRUE)
</code></pre>

<pre><code>* Verifying parameters...
* Applying division...
* Getting basic &#39;ddo&#39; attributes...
* Running map/reduce to get missing attributes...
* Getting basic &#39;ddo&#39; attributes...
</code></pre>

<p><code>divide()</code> must take an object of class &quot;ddo&quot; or &quot;ddf&quot;, and will return an object of either &quot;ddo&quot; or &quot;ddf&quot; (&quot;ddf&quot; if the resulting values are data frames).</p>

<p>Since the result of splitting the <code>iris</code> data by species is a data frame, <code>bySpecies</code> is now a &quot;ddf&quot; object.  We can inspect it with the following:</p>

<pre><code class="r">bySpecies
</code></pre>

<pre><code>
Distributed data object of class &#39;kvMemory&#39; with attributes: 

&#39;ddo&#39; attribute | value
----------------+-----------------------------------------------------------
 keys           | keys are available through getKeys(dat)
 totSize        | 10.9 KB
 nDiv           | 3
 splitSizeDistn | use splitSizeDistn(dat) to get distribution
 example        | use kvExample(dat) to get an example subset
 bsvInfo        | [empty] no BSVs have been specified

&#39;ddf&#39; attribute | value
----------------+-----------------------------------------------------------
 vars           | Sepal.Length(num), Sepal.Width(num), and 2 more
 transFn        | identity (original data is a data frame)
 nRow           | 150
 splitRowDistn  | use splitRowDistn(dat) to get distribution
 summary        | use summary(dat) to see summaries

Division:
  Type: Conditioning variable division
    Conditioning variables: Species

In-memory data connection
</code></pre>

<p>We see the same printout as we had with our manually-created division, with the addition of information about how the data was divided.</p>

<p>In the above example, conditioning variable division was specified with the <code>by</code> argument.  Here, simply specifying a character string or vector of character strings (for multiple conditioning variables) will invoke conditioning variable division.  A more formal way to achieve this is by using <code>condDiv()</code> to build the division specification:</p>

<pre><code class="r"># divide irisDdf by species using condDiv()
bySpecies &lt;- divide(irisDdf, by=condDiv(&quot;Species&quot;), update=TRUE)
</code></pre>

<pre><code>* Verifying parameters...
* Applying division...
* Getting basic &#39;ddo&#39; attributes...
* Running map/reduce to get missing attributes...
* Getting basic &#39;ddo&#39; attributes...
</code></pre>

<p>Using <code>condDiv()</code> is not necessary but follows the general idea of using a function to build a division specification that is and will be followed for other division methods.</p>

<p>Here&#39;s what a subset of the divide data looks like:</p>

<pre><code class="r"># look at a subset of bySpecies
bySpecies[[1]]
</code></pre>

<pre><code>[[1]]
[1] &quot;Species=setosa&quot;

[[2]]
   Sepal.Length Sepal.Width Petal.Length Petal.Width
1           5.1         3.5          1.4         0.2
2           4.9         3.0          1.4         0.2
 [ reached getOption(&quot;max.print&quot;) -- omitted 48 rows ]
</code></pre>

<p>Note that the &quot;Species&quot; column is missing in the value data frame.  This is because it is the variable we split on, and therefore has the same value for the entire subset.  All conditioning variables for a given subset are stored in a &quot;splitVars&quot; attribute, and can be retrieved by <code>getSplitVars()</code>:</p>

<pre><code class="r"># get the split variable (Species) for some subsets
getSplitVars(bySpecies[[1]])
</code></pre>

<pre><code>$Species
[1] &quot;setosa&quot;
</code></pre>

<pre><code class="r">getSplitVars(bySpecies[[2]])
</code></pre>

<pre><code>$Species
[1] &quot;versicolor&quot;
</code></pre>

<p>The keys for the division result are strings that specify how the data was divided:</p>

<pre><code class="r"># look at bySpecies keys
getKeys(bySpecies)
</code></pre>

<pre><code>$`4a7a45a288b320d14537ba28cfdb8db5`
[1] &quot;Species=setosa&quot;

$`88f9dd48ca10366898776cdf8fdc5418`
[1] &quot;Species=versicolor&quot;

$bc1f636f7428e8dc45d7d65ee4c78a45
[1] &quot;Species=virginica&quot;
</code></pre>

<h4>The <code>by</code> argument: <em>random replicate division</em></h4>

<p>Another way to divide data that is currently implemented is <em>random replicate</em> division.  For this, we use the division specification function <code>rrDiv()</code>.  This function allows you to specify the number of rows you would like each random subset to have, and optionally a random seed to use for the random assignment of rows to subsets.</p>

<p>Suppose we want to split the iris data into random subsets with roughly 10 rows per subset:</p>

<pre><code class="r"># divide iris data into random subsets of 10 rows per subset
set.seed(123)
byRandom &lt;- divide(bySpecies, by=rrDiv(10), update=TRUE)
</code></pre>

<pre><code>* Verifying parameters...
* Applying division...
* Getting basic &#39;ddo&#39; attributes...
* Running map/reduce to get missing attributes...
* Getting basic &#39;ddo&#39; attributes...
</code></pre>

<p>Note that we passed <code>bySpecies</code> as the input data.  We could just as well have specified <code>irisDdf</code> or any other division of the iris data.  The input partitioning doesn&#39;t matter.</p>

<pre><code class="r">byRandom
</code></pre>

<pre><code>
Distributed data object of class &#39;kvMemory&#39; with attributes: 

&#39;ddo&#39; attribute | value
----------------+-----------------------------------------------------------
 keys           | keys are available through getKeys(dat)
 totSize        | 24.46 KB
 nDiv           | 15
 splitSizeDistn | use splitSizeDistn(dat) to get distribution
 example        | use kvExample(dat) to get an example subset
 bsvInfo        | [empty] no BSVs have been specified

&#39;ddf&#39; attribute | value
----------------+-----------------------------------------------------------
 vars           | Sepal.Length(num), Sepal.Width(num), and 2 more
 transFn        | identity (original data is a data frame)
 nRow           | 150
 splitRowDistn  | use splitRowDistn(dat) to get distribution
 summary        | use summary(dat) to see summaries

Division:
  Type: Random replicate divison
    Approx. number of rows in each division: 10

In-memory data connection
</code></pre>

<p>We see there are still 150 rows (as there should be), but now there are 15 subsets.  </p>

<p>We can look at the distribution of the of the number of rows in each subset:</p>

<pre><code class="r"># plot distribution of the number of rows in each subset
plot(splitRowDistn(byRandom))
</code></pre>

<p><img src="figure/byrandom_row_distn.png" alt="plot of chunk byrandom_row_distn"> </p>

<p>We see that there are not exactly 10 rows in each subset, but 10 rows on average.  The random replicate algorithm simply randomly assigns each row of the input data into the number of bins \(K\) determined by the total number of rows \(n\) in the data divided by the desired number of rows per subset.  Thus the distribution of the number of rows in each subset is like a draw from a multinomial with number of trials \(n\) and event probabilities of being put into one of \(K\) bins as \(p_i=1/K, i=1, \ldots, K\).  We are working on a scalable approach to randomly assign exactly \(n/K\) rows to each subset.</p>

<p>The keys for random replicate divided data are simply labels indicating the bin:</p>

<pre><code class="r">getKeys(byRandom)
</code></pre>

<pre><code>$`1eba819cbcd7263e1862fa9d7d4be2d9`
[1] &quot;rr_1&quot;

$aa32d056aba0e16736e7a8a5c72d271e
[1] &quot;rr_10&quot;

$`2d67663e85cfc7d9211b1f66c04f4aca`
[1] &quot;rr_11&quot;

$`2f43184cb1e3e1313c04e1ea1b36297c`
[1] &quot;rr_12&quot;

$`8279729719e3b3c72580d43400e94725`
[1] &quot;rr_13&quot;

$`7a033d10212755ca848e3d49adb12cbd`
[1] &quot;rr_14&quot;

$`15e9c08531fcde946adf570a77665480`
[1] &quot;rr_15&quot;

$`2973567a41c2ec84cd5c9cc748ae9731`
[1] &quot;rr_2&quot;

$`2d4e834b37e65b87515e53b2024db8f8`
[1] &quot;rr_3&quot;

$`4a724f19d274e3186324e79429144fdb`
[1] &quot;rr_4&quot;

 [ reached getOption(&quot;max.print&quot;) -- omitted 5 entries ]
</code></pre>

<p>We will show an example of random replicate division in use later in this section.</p>

<h4>The <code>preTransFn</code> argument</h4>

<p><code>divide()</code> does not know how to break data into pieces unless it is dealing with data frames -- how would it know how to break up arbitrary data structures?  So the most <code>divide()</code>-friendly input data type is &quot;ddf&quot; objects.  But sometimes there is input data that is not &quot;ddf&quot; and we don&#39;t want to take the extra step of converting it to &quot;ddf&quot;.  Instead, we can pass a &quot;ddo&quot; object to <code>divide()</code> along with <code>preTransFn</code>, which applies a transformation function to each subset prior to division.  </p>

<p><code>preTransFn</code> can also be used when the input is &quot;ddf&quot; but we would like to make some change to it prior to division.  For example, suppose we only want <code>divide()</code> to operate on the <code>Sepal.Length</code> column:</p>

<pre><code class="r"># preTransFn to extract Sepal.Length from a value in a key-value pair
extractSepalLength &lt;- function(v)
   v[, c(&quot;Sepal.Length&quot;, &quot;Species&quot;)]
# test it on a subset to make sure it is doing what we want
kvApply(extractSepalLength, irisDdf[[1]])
</code></pre>

<pre><code>    Sepal.Length    Species
1            5.1     setosa
2            4.9     setosa
3            4.7     setosa
4            4.6     setosa
5            5.0     setosa
 [ reached getOption(&quot;max.print&quot;) -- omitted 145 rows ]
</code></pre>

<pre><code class="r"># apply division with preTransFn
bySpeciesSL &lt;- divide(irisDdf, by=&quot;Species&quot;, preTransFn=extractSepalLength)
</code></pre>

<pre><code>* Verifying parameters...
* Applying division...
* Getting basic &#39;ddo&#39; attributes...
</code></pre>

<p>It is always a good practice to test any function that operates on key-value pairs on a subset prior to running it over the entire data set, as we have done here (if you missed it, see the discussion on <a href="#key-value-pairs">applying functions to key-value pairs</a>).</p>

<p><code>preTransFn</code> is applied to each subset of the input data prior to dividing by species.  The result is a data frame of just <code>Sepal.Length</code> divided by species.</p>

<h4>Using <code>preTransFn</code> to create a derived conditioning variable</h4>

<p>A common use of <code>preTransFn</code> when the input data is a &quot;ddf&quot; object is to create a derived variable upon which we will perform division.  For example, suppose we would like to divide the iris data by both <code>Species</code> and a discretized version of <code>Sepal.Length</code>.</p>

<p>First, let&#39;s get a feel for the <code>Sepal.Length</code> variable:</p>

<pre><code class="r"># get summary statistics for Sepal.Length
summary(bySpecies)$Sepal.Length
</code></pre>

<pre><code>$nna
[1] 0

$stats
$stats$mean
[1] 5.843

$stats$var
[1] 0.6857

$stats$skewness
[1] 0.3118

$stats$kurtosis
[1] 2.426


$range
[1] 4.3 7.9
</code></pre>

<p>We see that its range is from 4.3 to 7.9.  Suppose we want to bin it by the integer.  We can create a new variable <code>slCut</code> by defining a <code>preTransFn</code> that adds this column to the data frame.  Then we specify that we want to divide the data both on <code>Species</code>, but also <code>slCut</code>, which doesn&#39;t exist in the input data but will exist in the data prior to division thanks to <code>preTransFn</code>.</p>

<pre><code class="r"># preTransFn to add a variable &quot;slCut&quot; of discretized Sepal.Length
sepalLengthCut &lt;- function(v) {
   v$slCut &lt;- cut(v$Sepal.Length, seq(0, 8, by=1))
   v
}
# test it on a subset
kvApply(sepalLengthCut, irisDdf[[1]])
</code></pre>

<pre><code>    Sepal.Length Sepal.Width Petal.Length Petal.Width    Species slCut
1            5.1         3.5          1.4         0.2     setosa (5,6]
2            4.9         3.0          1.4         0.2     setosa (4,5]
3            4.7         3.2          1.3         0.2     setosa (4,5]
 [ reached getOption(&quot;max.print&quot;) -- omitted 147 rows ]
</code></pre>

<pre><code class="r"># divide on Species and slCut
bySpeciesSL &lt;- divide(irisDdf, by=c(&quot;Species&quot;, &quot;slCut&quot;), 
   preTransFn = sepalLengthCut)
</code></pre>

<pre><code>* Verifying parameters...
* Applying division...
* Getting basic &#39;ddo&#39; attributes...
</code></pre>

<p>Since we added the variable <code>slCut</code> in our <code>preTransFn</code>, we can specify for divide to split on that variable.</p>

<p>Let&#39;s look at one subset:</p>

<pre><code class="r">bySpeciesSL[[3]]
</code></pre>

<pre><code>[[1]]
[1] &quot;Species=versicolor|slCut=(4,5]&quot;

[[2]]
  Sepal.Length Sepal.Width Petal.Length Petal.Width
1          4.9         2.4          3.3           1
2          5.0         2.0          3.5           1
3          5.0         2.3          3.3           1
</code></pre>

<p>As the key indicates, the species for this subset is &quot;versicolor&quot; and the sepal length is in the range <code>(4,5]</code>.  Recall that we can access the split variables for this subset with:</p>

<pre><code class="r">getSplitVars(bySpeciesSL[[3]])
</code></pre>

<pre><code>$Species
[1] &quot;versicolor&quot;

$slCut
[1] &quot;(4,5]&quot;
</code></pre>

<h4>The <code>postTransFn</code> argument</h4>

<p><code>postTransFn</code> provides a way for you to change the structure of the data after division, but prior to it being written to disk.  This can be used to get the data out of data frame mode (it must be a data frame just prior to and after division) or to subset or remove columns, etc.</p>

<h4>The &#39;spill&#39; argument</h4>

<p>Many times a conditioning variable division of interest will result in a long-tailed distribution of the data belonging to each subset, such that the data going into some subsets will get too large (remember that each subset must be small enough to be processed efficiently in memory).  The <code>spill</code> argument in <code>divide()</code> allows you to specify a limit to the number of rows that can belong in a subset, after which additional records will get &quot;spilled&quot; into a new subset.</p>

<p>For example, suppose we want no more than 12 rows per subset in our by-species division:</p>

<pre><code class="r"># divide iris data by species, spilling to new key-value after 12 rows
bySpeciesSpill &lt;- divide(irisDdf, by=&quot;Species&quot;, spill=12, update=TRUE)
</code></pre>

<pre><code>* Verifying parameters...
* Applying division...
* Getting basic &#39;ddo&#39; attributes...
* Running map/reduce to get missing attributes...
* Getting basic &#39;ddo&#39; attributes...
</code></pre>

<p>Let&#39;s see what our subsets look like now:</p>

<pre><code class="r"># look at some subsets
bySpeciesSpill[[1]]
</code></pre>

<pre><code>[[1]]
[1] &quot;Species=setosa_1&quot;

[[2]]
   Sepal.Length Sepal.Width Petal.Length Petal.Width
1           5.1         3.5          1.4         0.2
2           4.9         3.0          1.4         0.2
 [ reached getOption(&quot;max.print&quot;) -- omitted 10 rows ]
</code></pre>

<pre><code class="r">bySpeciesSpill[[5]]
</code></pre>

<pre><code>[[1]]
[1] &quot;Species=setosa_5&quot;

[[2]]
  Sepal.Length Sepal.Width Petal.Length Petal.Width
1          5.3         3.7          1.5         0.2
2          5.0         3.3          1.4         0.2
</code></pre>

<p>There are 5 different subsets for each species.  For example, &quot;Species=setosa&quot; has subset with keys: &quot;Species=setosa_1&quot;, ..., &quot;Species=setosa_5&quot;.  The first four subsets have 12 rows in each (each spilling into a new subset after it was filled with 12 rows), and the fifth subset has 2 rows, a total of 50 rows for &quot;Species=setosa&quot;.</p>

<h4>The &#39;filter&#39; argument</h4>

<p>The <code>filter</code> argument to <code>divide()</code> is an optional function that is applied to each candidate post-division key-value pair to determine whether it should be part of the resulting division.  A common case of when the <code>filter</code> argument is useful is when a division may result in a very large number of very small subsets and we are only interested in studying subsets with adequate size.</p>

<p>As an example, consider the iris splitting with <code>spill=12</code> from before.  Suppose that in addition to spilling records, we also only want to keep subsets that have more than 5 records in them.</p>

<pre><code class="r"># divide iris data by species, spill, and filter out subsets with &lt;=5 rows
bySpeciesFilter &lt;- divide(irisDdf, by=&quot;Species&quot;, spill=12,
   filter=function(v) nrow(v) &gt; 5, update=TRUE)
</code></pre>

<pre><code>* Verifying parameters...
* Applying division...
* Getting basic &#39;ddo&#39; attributes...
* Running map/reduce to get missing attributes...
* Getting basic &#39;ddo&#39; attributes...
</code></pre>

<pre><code class="r">bySpeciesFilter
</code></pre>

<pre><code>
Distributed data object of class &#39;kvMemory&#39; with attributes: 

&#39;ddo&#39; attribute | value
----------------+-----------------------------------------------------------
 keys           | keys are available through getKeys(dat)
 totSize        | 30.79 KB
 nDiv           | 12
 splitSizeDistn | use splitSizeDistn(dat) to get distribution
 example        | use kvExample(dat) to get an example subset
 bsvInfo        | [empty] no BSVs have been specified

&#39;ddf&#39; attribute | value
----------------+-----------------------------------------------------------
 vars           | Sepal.Length(num), Sepal.Width(num), and 2 more
 transFn        | identity (original data is a data frame)
 nRow           | 144
 splitRowDistn  | use splitRowDistn(dat) to get distribution
 summary        | use summary(dat) to see summaries

Division:
  Type: Conditioning variable division
    Conditioning variables: Species

In-memory data connection
</code></pre>

<p>The <code>filter</code> function simply returns <code>TRUE</code> if we want to keep the subset and <code>FALSE</code> if not.</p>

<p>Now we have 144 rows and 12 divisions - the 3 subsets with 2 rows were omitted from the result.</p>

<p>Note that the filter is applied to the data prior to the application of <code>postTransFn</code>.</p>

</div>


<div class='tab-pane' id='recombination'>
<h3>Recombination</h3>

<p>In this section, we cover basic usage of the <code>recombine()</code> method.  The function documentation is available <a href="http://hafen.github.io/datadr/functionref.html#recombine">here</a>.</p>

<p><code>recombine()</code> can be applied to any &quot;ddo&quot; or &quot;ddf&quot; object.  It is not necessary to give it an object that was created using <code>divide()</code>, but some recombination methods may not be valid on just any input (such as recombinations that assume random replicate division).</p>

<p>Aside from the <code>data</code> argument (which of course is the input &quot;ddo&quot; or &quot;ddf&quot; object), the main arguments in <code>recombine()</code> are <code>apply</code> and <code>combine</code>.  We will see several examples of different usage throughout this section, but first we provide an overview of the <code>apply</code> and <code>combine</code> arguments.</p>

<h4><code>apply</code> argument</h4>

<p>The <code>apply</code> argument can either be a user-defined function that will be applied to each key-value pair, or a built-in <code>apply</code> function for performing specific tasks.  Custom <code>apply</code> functions currently implemented include:</p>

<ul>
<li><code>drGLM()</code>: a proof-of-concept for fitting generalized linear models in the D&amp;R paradigm</li>
<li><code>drBLB()</code>: a proof-of-concept implementation of the bag of little bootstraps, which falls under the D&amp;R paradigm.</li>
</ul>

<p>The <code>combine</code> argument tells <code>recombine()</code> how to collate the results of <code>apply</code>.  Currently available <code>combine</code> methods include</p>

<ul>
<li><code>combCollect()</code>: (the default) - returns a list of key-value pairs of the result of <code>apply</code> being applied to each subset</li>
<li><code>combRbind()</code>: for <code>apply</code> functions that return data frames, this <code>combine</code> method rbinds all of the results into a single data frame</li>
<li><code>combMeanCoef()</code>: expects each </li>
</ul>

<p>Most <code>recombine()</code> operations currently are done with a custom <code>apply</code> function using either <code>combCollect()</code> or <code>combRbind()</code> to combine the results.  We will see examples of these throughout this section.</p>

<p>Much of the anticipated future work for <code>datadr</code> is the construction of several <code>apply</code>-<code>combine</code> pairs that are useful for different analysis tasks.  The apply/combine pairs <code>drGLM()</code>-<code>combMeanCoef()</code> and <code>drBLB()</code>-<code>combMeanCoef()</code> are two initial examples.</p>

<h4>Simple <code>lapply()</code>-like recombination</h4>

<p>Some of the most common uses of <code>recombine()</code> are simple <code>lapply()</code>-like operations, where we simply want to apply a function to each subset and pull the results back in.</p>

<p>For example, suppose we would like to compute the mean petal width for each species in our <code>bySpecies</code> division:</p>

<pre><code class="r">recombine(bySpecies, apply=function(v) mean(v$Petal.Width))
</code></pre>

<pre><code>* Verifying suitability of &#39;apply&#39; for division type...
* Verifying suitability of &#39;output&#39; for specified &#39;combine&#39;...
* Testing the division method on a subset...
* Applying to all subsets...
* Getting basic &#39;ddo&#39; attributes...
</code></pre>

<pre><code>[[1]]
[[1]][[1]]
[1] &quot;Species=setosa&quot;

[[1]][[2]]
[1] 0.246


[[2]]
[[2]][[1]]
[1] &quot;Species=versicolor&quot;

[[2]][[2]]
[1] 1.326


[[3]]
[[3]][[1]]
[1] &quot;Species=virginica&quot;

[[3]][[2]]
[1] 2.026
</code></pre>

<p>Here, the default <code>combCollect()</code> was used to combine the results, giving us a list of key-value pairs with the value being the mean petal width.</p>

<p>Suppose we would like the result to be a data frame.  We can do this with <code>combRbind()</code>:</p>

<pre><code class="r">recombine(bySpecies, apply=function(v) mean(v$Petal.Width), comb=combRbind())
</code></pre>

<pre><code>* Verifying suitability of &#39;apply&#39; for division type...
* Verifying suitability of &#39;output&#39; for specified &#39;combine&#39;...
* Testing the division method on a subset...
* Applying to all subsets...
* Getting basic &#39;ddo&#39; attributes...
</code></pre>

<pre><code>     Species   val
1     setosa 0.246
2 versicolor 1.326
3  virginica 2.026
</code></pre>

<p>The <code>apply</code> function here returns a scalar which is coerced into a data frame.  Note that by default if the input data keys are characters, they will be added to the data frame.  Here is another example of using <code>combRbind()</code> with an <code>apply</code> function that returns a data frame:</p>

<pre><code class="r">meanApply &lt;- function(v) {
   data.frame(mpw=mean(v$Petal.Width), mpl=mean(v$Petal.Length))
}
recombine(bySpecies, apply=meanApply, comb=combRbind())
</code></pre>

<pre><code>* Verifying suitability of &#39;apply&#39; for division type...
* Verifying suitability of &#39;output&#39; for specified &#39;combine&#39;...
* Testing the division method on a subset...
* Applying to all subsets...
* Getting basic &#39;ddo&#39; attributes...
</code></pre>

<pre><code>     Species   mpw   mpl
1     setosa 0.246 1.462
2 versicolor 1.326 4.260
3  virginica 2.026 5.552
</code></pre>

<p>Sometimes we would like the result to be a new division object, in which case we use <code>combDdo()</code> </p>

<pre><code class="r">recombine(bySpecies, apply=meanApply, comb=combDdo())
</code></pre>

<pre><code>* Verifying suitability of &#39;apply&#39; for division type...
* Verifying suitability of &#39;output&#39; for specified &#39;combine&#39;...
* Testing the division method on a subset...
* Applying to all subsets...
* Getting basic &#39;ddo&#39; attributes...
</code></pre>

<pre><code>
Distributed data object of class &#39;kvMemory&#39; with attributes: 

&#39;ddo&#39; attribute | value
----------------+-----------------------------------------------------------
 keys           | keys are available through getKeys(dat)
 totSize        | 5.37 KB
 nDiv           | 3
 splitSizeDistn | [empty] call updateAttributes(dat) to get this value
 example        | use kvExample(dat) to get an example subset
 bsvInfo        | [empty] no BSVs have been specified

In-memory data connection
</code></pre>

<div class="alert alert-warning">
Note: at this point, those familiar with <code>lapply()</code>, <code>tapply()</code>, etc., the <code>plyr</code> and <code>dplyr</code> packages, and other similar approaches might wondering what the difference is with `datadr`.  For notes on this, see the <a href="#faq">FAQ</a>.
</div>

<!-- #### Note about output from `recombine()`

Although `recombine()` typically returns a reduced amount of data, sometimes the result is too large to read back into memory, and sometimes we would like the result to be a new "ddo" dataset to apply further recombinations on.

When the result of a recombine is large, we can store  -->

</div>


<div class='tab-pane' id='between-subset-variables'>
<h3>Between-Subset Variables</h3>

<p>A useful thing to do when creating a division is to specify <em>between subset variables</em>.  More to come here...</p>

<hr>

</div>


<div class='tab-pane' id='dr-examples'>
<h3>D&amp;R Examples</h3>

<p>Here are some examples with a new (but still small) data set that illustrate some general use of division and recombination including the use of random replicate division and some different recombination methods to fit a GLM to a dataset.  </p>

<p>Although there are different approaches for in-memory data like this one, we will use <code>datadr</code> tools to deal with the data throughout, again remembering that these tools scale.</p>

<h4>The data</h4>

<p>The data is adult income from the 1994 census database, pulled from the <a href="http://archive.ics.uci.edu/ml/datasets/Adult">UCI machine learning repository</a>.  See <code>?adult</code> for more details.</p>

<p>First, we load the data (available as part of the <code>datadr</code> package) and turn it into a &quot;ddf&quot; object:</p>

<pre><code class="r">data(adult)
# turn adult into a ddf
adultDdf &lt;- ddf(adult, update=TRUE)
</code></pre>

<pre><code>* Getting basic &#39;ddo&#39; attributes...
* Getting basic &#39;ddf&#39; attributes...
* Running map/reduce to get missing attributes...
* Getting basic &#39;ddo&#39; attributes...
</code></pre>

<pre><code class="r">adultDdf
</code></pre>

<pre><code>
Distributed data object of class &#39;kvMemory&#39; with attributes: 

&#39;ddo&#39; attribute | value
----------------+-----------------------------------------------------------
 keys           | keys are available through getKeys(dat)
 totSize        | 2.12 MB
 nDiv           | 1
 splitSizeDistn | use splitSizeDistn(dat) to get distribution
 example        | use kvExample(dat) to get an example subset
 bsvInfo        | [empty] no BSVs have been specified

&#39;ddf&#39; attribute | value
----------------+-----------------------------------------------------------
 vars           | age(int), workclass(fac), fnlwgt(int), and 13 more
 transFn        | identity (original data is a data frame)
 nRow           | 32561
 splitRowDistn  | use splitRowDistn(dat) to get distribution
 summary        | use summary(dat) to see summaries

In-memory data connection
</code></pre>

<pre><code class="r">#look at the names
names(adultDdf)
</code></pre>

<pre><code> [1] &quot;age&quot;          &quot;workclass&quot;    &quot;fnlwgt&quot;       &quot;education&quot;   
 [5] &quot;educationnum&quot; &quot;marital&quot;      &quot;occupation&quot;   &quot;relationship&quot;
 [9] &quot;race&quot;         &quot;sex&quot;         
 [ reached getOption(&quot;max.print&quot;) -- omitted 6 entries ]
</code></pre>

<p>We see that there are about 32K observations, and we see the various variables available.</p>

<p>We&#39;ll start with some simple exploratory analysis.  One variable of interest in the data is education.  We can look at the summary statistics to see the frequency distribution of <code>education</code> (which were computed since we specified <code>update=TRUE</code> when we created <code>adultDdf</code>):</p>

<pre><code class="r">library(lattice)
edTable &lt;- summary(adultDdf)$education$freqTable
edTable$var &lt;- with(edTable, reorder(var, Freq, mean))
dotplot(var ~ Freq, data=edTable)
</code></pre>

<p><img src="figure/ed_table.png" alt="plot of chunk ed_table"> </p>

<h4>Division by education group</h4>

<p>Perhaps we would like to divide our data by <code>education</code> and investigate how some of the other variables behave within education.  </p>

<p>Suppose we want to make some changes to the <code>education</code> variable: we want to leave out &quot;Preschool&quot; and create groups &quot;Some-elementary&quot;, &quot;Some-middle&quot;, and &quot;Some-HS&quot;.  Of course in a real analysis you would probably want to first make sure you aren&#39;t washing any interesting effects out by making these groupings.</p>

<p>We can handle these changes to the <code>education</code> variable using <code>preTransFn</code> in our call to <code>divide()</code>.  You might be wondering why not make the changes to the variable in the original data frame prior to doing all of this.  For this example, of course we can do that, but suppose this data were, say, 1TB in size.  You would probably much rather apply the transformation during the division than create a new set of data.</p>

<p>The following transformation function will achieve the desired result:</p>

<pre><code class="r"># make a preTransFn to group some education levels
edGroups &lt;- function(v) {
   v$edGroup &lt;- as.character(v$education)
   v$edGroup[v$edGroup %in% c(&quot;1st-4th&quot;, &quot;5th-6th&quot;)] &lt;- &quot;Some-elementary&quot;
   v$edGroup[v$edGroup %in% c(&quot;7th-8th&quot;, &quot;9th&quot;)] &lt;- &quot;Some-middle&quot;
   v$edGroup[v$edGroup %in% c(&quot;10th&quot;, &quot;11th&quot;, &quot;12th&quot;)] &lt;- &quot;Some-HS&quot;
   v
}
</code></pre>

<p>This adds a variable <code>edGroup</code> with the desired grouping of education levels.  We can now divide the data by <code>edGroup</code>.  We specify a <code>filterFn</code> to only allow data to be output that does not correspond to &quot;Preschool&quot;.</p>

<pre><code class="r"># divide by edGroup and filter out &quot;Preschool&quot;
byEdGroup &lt;- divide(adultDdf, by=&quot;edGroup&quot;, 
   preTransFn=edGroups, 
   filterFn=function(x) x$edGroup[1] != &quot;Preschool&quot;,
   update=TRUE)
</code></pre>

<pre><code>* Verifying parameters...
* Applying division...
</code></pre>

<pre><code>Warning: closing unused connection 7 (&lt;-localhost:11966)
Warning: closing unused connection 6 (&lt;-localhost:11966)
Warning: closing unused connection 5 (&lt;-localhost:11966)
</code></pre>

<pre><code>* Getting basic &#39;ddo&#39; attributes...
* Running map/reduce to get missing attributes...
* Getting basic &#39;ddo&#39; attributes...
</code></pre>

<pre><code class="r">byEdGroup
</code></pre>

<pre><code>
Distributed data object of class &#39;kvMemory&#39; with attributes: 

&#39;ddo&#39; attribute | value
----------------+-----------------------------------------------------------
 keys           | keys are available through getKeys(dat)
 totSize        | 3.3 MB
 nDiv           | 11
 splitSizeDistn | use splitSizeDistn(dat) to get distribution
 example        | use kvExample(dat) to get an example subset
 bsvInfo        | [empty] no BSVs have been specified

&#39;ddf&#39; attribute | value
----------------+-----------------------------------------------------------
 vars           | age(int), workclass(cha), fnlwgt(int), and 13 more
 transFn        | identity (original data is a data frame)
 nRow           | 32510
 splitRowDistn  | use splitRowDistn(dat) to get distribution
 summary        | use summary(dat) to see summaries

Division:
  Type: Conditioning variable division
    Conditioning variables: edGroup

In-memory data connection
</code></pre>

<p>We can look at the distribution of number of people in each education group with the following simple recombination:</p>

<pre><code class="r"># tabulate number of people in each education group
edGroupTable &lt;- recombine(byEdGroup, apply=nrow, combine=combRbind())
</code></pre>

<pre><code>* Verifying suitability of &#39;apply&#39; for division type...
* Verifying suitability of &#39;output&#39; for specified &#39;combine&#39;...
* Testing the division method on a subset...
* Applying to all subsets...
* Getting basic &#39;ddo&#39; attributes...
</code></pre>

<pre><code class="r">edGroupTable
</code></pre>

<pre><code>           edGroup   val
1       Assoc-acdm  1067
2        Assoc-voc  1382
3        Bachelors  5355
4        Doctorate   413
5          HS-grad 10501
 [ reached getOption(&quot;max.print&quot;) -- omitted 6 rows ]
</code></pre>

<p>A similar dotplot as before can be made with this data.</p>

<h4>Investigating data by education group</h4>

<p>There are many things we might be interested in doing with our <code>byEdGroup</code> division.  We&#39;ll just show one quick example.</p>

<p>One thing we might be interested in is how different the distribution of gender is within each of the education groups.  One way to do this is to look at the ratio of men to women.  We can compute this ratio with a simple recombination:</p>

<pre><code class="r"># compute male/female ratio by education group
sexRatio &lt;- recombine(byEdGroup, apply=function(x) {
   tab &lt;- table(x$sex)
   data.frame(maleFemaleRatio=tab[&quot;Male&quot;] / tab[&quot;Female&quot;])
}, combine=combRbind())
</code></pre>

<pre><code>* Verifying suitability of &#39;apply&#39; for division type...
* Verifying suitability of &#39;output&#39; for specified &#39;combine&#39;...
* Testing the division method on a subset...
* Applying to all subsets...
* Getting basic &#39;ddo&#39; attributes...
</code></pre>

<pre><code class="r">sexRatio
</code></pre>

<pre><code>           edGroup maleFemaleRatio
1       Assoc-acdm           1.534
2        Assoc-voc           1.764
3        Bachelors           2.308
4        Doctorate           3.802
5          HS-grad           2.098
 [ reached getOption(&quot;max.print&quot;) -- omitted 6 rows ]
</code></pre>

<p>We can visualize it with the following:</p>

<pre><code class="r"># make dotplot of male/female ratio by education group
sexRatio$edGroup &lt;- with(sexRatio, reorder(edGroup, maleFemaleRatio, mean))
dotplot(edGroup ~ maleFemaleRatio, data=sexRatio)
</code></pre>

<p><img src="figure/vis_sexratio.png" alt="plot of chunk vis_sexratio"> </p>

<p>We know the marginal distribution of gender is lopsided to begin with (see <code>summary(byEdGroup)$sex</code>), but we don&#39;t know if the sample we are dealing with is biased or not...  There are obviously many many directions to go with the exploratory analysis and hopefully these few examples provide a start and a feel for how to go about</p>

<p>One more thing to note about what we have done so far:  We have shown a couple of examples of using <code>datadr</code> to summarize the data in different ways and visualize the summaries.  This is a good thing to do.  But we also want to be able to visualize the subsets in detail.  For example, we might want to look at a <code>scatterplot</code> of <code>age</code> vs. <code>hoursperweek</code>.  With this small data set, we obviously can pull all subsets in and make a lattice plot or faceted ggplot.  However, what if there are thousands or hundreds of thousands of subsets?  This is where the <a href="http://hafen.github.io/trelliscope/">trelliscope</a> package -- a visualization companion to <code>datadr</code> -- comes in.</p>

<h4>Fitting a GLM to the data</h4>

<p>Although the majority of the work we do is quite effective through clever use of generic division and recombination approaches and making heavy use of visualization, it is worthwhile to show some of the approaches of approximating all-data estimates with <code>datadr</code>.</p>

<p>Therefore, we now turn to some examples of ways to apply analytical methods across the entire dataset from within the D&amp;R paradigm.  For example, suppose we would like to model the dependence of making more or less than 50K per year on <code>educationnum</code>, <code>hoursperweek</code>, and <code>sex</code> using logistic regression.</p>

<p>Before doing it with <code>datadr</code>, let&#39;s first apply the method to the original data frame, so that we can compare the results.  Recall again that since this is a small data set, we can do things the &quot;usual&quot; way:</p>

<pre><code class="r"># fit a glm to the original adult data frame
rglm &lt;- glm(incomebin ~ educationnum + hoursperweek + sex, data=adult, family=binomial())
summary(rglm)
</code></pre>

<pre><code>
Call:
glm(formula = incomebin ~ educationnum + hoursperweek + sex, 
    family = binomial(), data = adult)

Deviance Residuals: 
   Min      1Q  Median      3Q     Max  
-2.456  -0.705  -0.429  -0.140   3.115  

Coefficients:
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  -7.23765    0.09520   -76.0   &lt;2e-16 ***
educationnum  0.35785    0.00654    54.7   &lt;2e-16 ***
hoursperweek  0.03299    0.00126    26.2   &lt;2e-16 ***
sexMale       1.21167    0.03679    32.9   &lt;2e-16 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 35948  on 32560  degrees of freedom
Residual deviance: 29472  on 32557  degrees of freedom
AIC: 29480

Number of Fisher Scoring iterations: 5
</code></pre>

<!-- tmp <- predict(rglm)
tmp[tmp < 0.5] <- 0
tmp[tmp > 0.5] <- 1

length(which(tmp == adult$income)) / nrow(adult) -->

<p>Now let&#39;s compare this to a few <code>datadr</code> approaches.  Note that these approaches are currently proof-of-concept only and are meant to illustrate ideas.  We will illustrate <code>drGLM()</code> and <code>drBLB()</code>.</p>

<h4>Fitting a GLM with <code>drGLM()</code></h4>

<p>For the results of <code>drGLM()</code> and <code>drBLB()</code> to be valid, we need a random-replicate division of the data.  We will choose a division that provides about 500 rows in each subset and that only has the variables that we care about:</p>

<pre><code class="r">rrAdult &lt;- divide(adultDdf, by=rrDiv(500), update=TRUE,
   postTrans=function(x) 
      x[,c(&quot;incomebin&quot;, &quot;educationnum&quot;, &quot;hoursperweek&quot;, &quot;sex&quot;)])
</code></pre>

<pre><code>* Verifying parameters...
* Applying division...
* Getting basic &#39;ddo&#39; attributes...
* Running map/reduce to get missing attributes...
* Getting basic &#39;ddo&#39; attributes...
</code></pre>

<!-- plot(splitRowDistn(rrAdult)) -->

<p>Now, we can call <code>recombine()</code> with <code>apply=drGLM(...)</code>.  <code>drGLM()</code> has been designed to take any arguments you might pass to <code>glm()</code> and pass them on to the function being applied to each subset.  We use <code>combine=combMeanCoef()</code>, which is a function that has been designed specifically to take coefficient results from model fits applied to each subset and average them:</p>

<pre><code class="r">recombine(
   data = rrAdult, 
   apply = drGLM(incomebin ~ educationnum + hoursperweek + sex, 
      family = binomial()), 
   combine = combMeanCoef())
</code></pre>

<pre><code>* Verifying suitability of &#39;apply&#39; for division type...
* Verifying suitability of &#39;output&#39; for specified &#39;combine&#39;...
* Testing the division method on a subset...
* Applying to all subsets...
* Getting basic &#39;ddo&#39; attributes...
</code></pre>

<pre><code> (Intercept) educationnum hoursperweek      sexMale 
    -7.35868      0.36195      0.03409      1.23762 
</code></pre>

<p>Note you this operation could be done manually without too much additional effort with a user-defined apply function.  Since <code>glm()</code> is a common function, we wrapped it with <code>drGLM()</code> to make things simpler.</p>

<p>If we compare the result to the all-data estimate, the values are close.  However, with this approach, we do not get any inference about the estimates.</p>

<h4>Fitting a GLM with <code>drBLB()</code></h4>

<p>We can use the bag of little bootstraps (BLB) approach to fit a GLM to the data.  The idea of bag of little bootstraps is to split the data into random subsets and apply a bootstrap method to each subset, compute a bootstrap metric to the result, and then average the metric across all subsets.</p>

<p>One important thing to keep in mind is that BLB requires each subset be resampled with with \(N\) replications, \(N\) being the total number of rows in the entire data set.  Since each subset has much fewer than \(N\) rows, say \(n\), we can imitate taking \(N\) draws by sampling from a multinomial with \(n\) bins with uniform probability and assigning weights to each of the \(n\) observations in the subset and computing weights from these and passing that as the <code>weights</code> argument to <code>glm()</code>.  Any R method that meets BLB requirements and accommodates this sampling scheme in one way or another can be used with <code>drBLB()</code>.</p>

<p>We apply <code>drBLB()</code> to each subset, specifying the <code>statistic</code> to be computed for each bootstrap sample, the <code>metric</code> to compute on the statistics, and the number of bootstrap replications <code>R</code>.  We also need to tell it the total number of rows in the data set.  Right now, <code>drBLB()</code> simply returns a numeric vector, which is combined using <code>combMean()</code>.</p>

<pre><code class="r">recombine(rrAdult,
   apply=drBLB(
      statistic = function(x, weights)
         coef(glm(incomebin ~ educationnum + hoursperweek + sex, 
            data=x, weights=weights, family=binomial())),
      metric = function(x)
         quantile(x, c(0.05, 0.95)),
      R = 100,
      n = nrow(rrAdult)
   ),
   combine=combMean()
)
</code></pre>

<pre><code>* Verifying suitability of &#39;apply&#39; for division type...
* Verifying suitability of &#39;output&#39; for specified &#39;combine&#39;...
* Testing the division method on a subset...
* Applying to all subsets...
* Getting basic &#39;ddo&#39; attributes...
</code></pre>

<pre><code>[1] -7.52217 -7.20250  0.35162  0.37333  0.03200  0.03612  1.17694  1.29583
</code></pre>

<p>The result here is simply a vector, where each successive pair of elements represents the lower and upper 95% confidence limit for <code>intercept</code>, <code>educationnum</code>, <code>hoursperweek</code>, and <code>sexMale</code>.  Close inspection shows that they are similar to what is returned from the all-data <code>glm()</code> estimate and that confidence interval widths are about the same.</p>

<!-- # BLB result: (500 per subset)
-7.51194450 -7.20129785  
0.35235341  0.37403775  
0.03162063  0.03572909  
1.17855898 1.29983445

-7.53768498 x -7.22450246  (0.3132) (compare to 0.3732)
0.35235099 x 0.37423944    (0.0219) (compare to 0.0256)
0.03217393 x 0.03638794    (0.0042) (compare to 0.0049)
1.17440023 x 1.29530486    (0.1209) (compare to 0.1442)

# BLB result: (200 per subset)
-7.74911838 -7.42470919  (0.3244) (compare to 0.3732)
0.36184991  0.38391722  (0.0221) (compare to 0.0256)
0.03323966 0.03743492  (0.0042) (compare to 0.0049)
1.21932545  1.34476423  (0.1254) (compare to 0.1442)

# glm result:
-7.237654   0.095202
 0.357855   0.006541
 0.032991   0.001257
 1.211674   0.036791 -->

<!-- # drGLM result (500 per subset)
(Intercept) educationnum hoursperweek     sex Male 
-7.35496959   0.36326367   0.03365385   1.23792439 -->

</div>


<div class='tab-pane' id='introduction-to-mapreduce'>
<h3>Introduction to MapReduce</h3>

<p>MapReduce is a simple but powerful programming model for breaking a task into pieces and operating on those pieces in an embarrassingly parallel manner across a cluster.  The approach was popularized by Google (Dean &amp; Ghemawat, 2008).  </p>

<p>MapReduce forms the basis of all <code>datadr</code> operations.  While the goal of <code>datadr</code> is for the higher-level <code>divide()</code> and <code>recombine()</code> methods to take care of all analysis needs, there may be times that the user would like to write MapReduce code directly.  <code>datadr</code> exposes general MapReduce interface that runs over any implemented backend.  The most popular of these, of course, is RHIPE.</p>

<h4>MapReduce overview</h4>

<p>MapReduce operates on key-value pairs.  The input, output, and intermediate data are all key-value pairs.  A MapReduce job consists of three phases that operate on these key-value pairs: the <em>map</em>, the <em>shuffle/sort</em>, and the <em>reduce</em>:</p>

<ul>
<li><strong>Map</strong>: A map function is applied to each input key-value pair, which does some user-defined processing and emits new key-value pairs to intermediate storage to be processed by the reduce.</li>
<li><strong>Shuffle/Sort</strong>: The map output values are collected for each unique map output key and passed to a reduce function.</li>
<li><strong>Reduce</strong>: A reduce function is applied in parallel to all values corresponding to each unique map output key and emits output key-value pairs.</li>
</ul>

<p>A simple schematic of this is shown below.</p>

<p><img src="image/mroverview.svg" width="450px" alt="mroverview" style="display:block; margin:auto"/></p>

<!-- ![mroverview](image/mroverview.png) -->

<p>The map function and reduce function are user-defined.  The MapReduce engine takes care of everything else.  We will get a better feel for how things work by looking at some examples in this section.</p>

<h4>Iris data (again)</h4>

<p>We will illustrate MapReduce by continuing to look at the <code>iris</code> data.  This time, we&#39;ll split it randomly into 4 key-value pairs:</p>

<pre><code class="r"># split iris data randomly into 4 key-value pairs
set.seed(1234)
ind &lt;- split(sample(1:150), sample(1:4, 150, replace=TRUE))
irisRKV &lt;- lapply(seq_along(ind), function(i) {
   list(i, iris[ind[[i]], c(&quot;Petal.Length&quot;, &quot;Species&quot;)])
})
irisRKV
</code></pre>

<pre><code>[[1]]
[[1]][[1]]
[1] 1

[[1]][[2]]
    Petal.Length    Species
93           4.0 versicolor
91           4.4 versicolor
126          6.0  virginica
127          4.8  virginica
138          5.5  virginica
 [ reached getOption(&quot;max.print&quot;) -- omitted 23 rows ]


[[2]]
[[2]][[1]]
[1] 2

[[2]][[2]]
    Petal.Length    Species
18           1.4     setosa
95           4.2 versicolor
98           4.3 versicolor
40           1.5     setosa
39           1.3     setosa
 [ reached getOption(&quot;max.print&quot;) -- omitted 38 rows ]


[[3]]
[[3]][[1]]
[1] 3

[[3]][[2]]
    Petal.Length    Species
2            1.4     setosa
73           4.9 versicolor
76           4.4 versicolor
114          5.0  virginica
136          6.1  virginica
 [ reached getOption(&quot;max.print&quot;) -- omitted 38 rows ]


[[4]]
[[4]][[1]]
[1] 4

[[4]][[2]]
    Petal.Length    Species
92           4.6 versicolor
149          5.4  virginica
34           1.4     setosa
25           1.9     setosa
42           1.3     setosa
 [ reached getOption(&quot;max.print&quot;) -- omitted 31 rows ]
</code></pre>

<p>All inputs and outputs to MapReduce jobs in <code>datadr</code> are &quot;ddo&quot; or &quot;ddf&quot; objects.  Hence, we turn <code>irisRKV</code> into a &quot;ddf&quot; object:</p>

<pre><code class="r"># represent irisRKV as a distributed data frame
irisRddf &lt;- ddf(irisRKV)
</code></pre>

<pre><code>* Getting basic &#39;ddo&#39; attributes...
* Getting basic &#39;ddf&#39; attributes...
</code></pre>

</div>


<div class='tab-pane' id='mapreduce-with-datadr'>
<h3>MapReduce with datadr</h3>

<p>MapReduce jobs are executed in <code>datadr</code> with a call to <code>mrExec()</code>.  The main inputs a user should be concerned with are:</p>

<ul>
<li><code>data</code>: an object of class &quot;ddf&quot; or &quot;ddo&quot;</li>
<li><code>map</code>: an R expression that is evaluated during the map stage</li>
<li><code>reduce</code>: a vector of R expressions with names <code>pre</code>, <code>reduce</code>, and <code>post</code> that is evaluated during the reduce stage</li>
</ul>

<p>Other inputs of interest are the following:</p>

<ul>
<li><code>setup</code>:  an expression of R code to be run before <code>map</code> and <code>reduce</code></li>
<li><code>output</code>: a &quot;kvConnection&quot; object indicating where the output data should reside -- see <a href="#backend-choices">Store/Compute Backends</a></li>
<li><code>control</code>: parameters specifying how the backend should handle things (most-likely parameters to rhwatch in RHIPE) -- see <a href="#backend-choices">Store/Compute Backends</a></li>
<li><code>params</code>: a named list of parameters external to the input data that are needed in the map or reduce phases</li>
</ul>

<p>In <code>datadr</code>, the <code>map</code> and <code>reduce</code> must be specified by the user as an R expression.  An input dataset, which is a &quot;ddo&quot; or &quot;ddf&quot; object, </p>

<h4>The <code>map</code> expression</h4>

<p>The map expression is simply an R expression that operates on a chunk of input key-value pairs.  Map expressions operate in parallel on disjoint chunks of the input data.  For example, if there are 1000 input key-value pairs of roughly equal size and there are 5 map tasks running, then each map task will operate on around 200 key-value pairs over the course of the job.  Depending on the size of each key-value pair, typically a map task will operate on batches of these key-value pairs, say 10 at a time, until all 200 have been processed.</p>

<p>A <code>datadr</code> map expression has the following essential objects or functions available:</p>

<ul>
<li><code>map.keys</code>: a list of the current block of input keys</li>
<li><code>map.values</code>: a list of the current block of input values</li>
<li><code>collect()</code>: a function that emits key-value pairs to the shuffle/sort process</li>
</ul>

<p>A map expression skeleton would look like this:</p>

<pre><code class="r">map &lt;- expression({
   # do some operations on map.keys and map.values
   # emit result to shuffle/sort using collect(key, value)
})
</code></pre>

<p>A key element of the map expression is the <code>collect()</code> function, which passes modified key-value pairs to the shuffle/sort phase prior to the reduce. The first argument of the function is a key, and the second is a value. When you have reached a point in your map expression that you are ready to pass the current processed key-value pair to the reducer, you call <code>collect()</code>.</p>

<h4>The <code>reduce</code> expression</h4>

<p>The reduce expression is processed for each set of unique keys emitted from the running the map expression over the data.  It consists of <code>pre</code>, <code>reduce</code> and <code>post</code> expressions.</p>

<p>A <code>datadr</code> reduce expression has the following essential objects or functions available:</p>

<ul>
<li><code>reduce.key</code>: a unique map output key</li>
<li><code>reduce.values</code>: a collection of all of the map output keys the correspond to <code>reduce.key</code></li>
<li><code>collect()</code>: a function that emits key-value pairs to the output dataset</li>
</ul>

<p>For example, say we have a map that emitted key-value pairs: <code>(&quot;a&quot;, 1)</code>, <code>(&quot;a&quot;, 2)</code>, and <code>(&quot;a&quot;, 3)</code>.  The shuffle/sort gathers all map outputs with key <code>&quot;a&quot;</code> and sets <code>reduce.key = &quot;a&quot;</code> and <code>reduce.values = list(1, 2, 3)</code>.</p>

<p>Note that in many cases, there are a very large number of <code>reduce.values</code> such that we must iterate through batches of them.  This is the purpose of the <code>pre</code>, <code>reduce</code>, and <code>post</code> parts of the reduce expression.  In the <code>pre</code>, we might initialize a result object.  Then the <code>reduce</code> part might get called multiple times until all <code>reduce.values</code> have been passed through.  Finally, we can post-process the result object and emit it to the output data in the <code>post</code> part of the expression.  (Note that we can emit output at any place in the reduce expression, but this is typically how it is done.)</p>

<p>A reduce expression skeleton would look like this:</p>

<pre><code class="r">reduce &lt;- expression(
   pre = {
      # initialize objects in which results will be stored
   },
   reduce = {
      # take current batch of reduce.values and update the result
   },
   post = {
      # emit output key-value pairs using collect(key, value)
   }
)
</code></pre>

<p>We will now solidify how these are used with some examples.</p>

</div>


<div class='tab-pane' id='mapreduce-examples'>
<h3>MapReduce Examples</h3>

<!-- 
k <- irisDdf[[1]][[1]]
v <- irisDdf[[1]][[2]] 
-->

<!-- 
lapply(irisKV, function(x) max(x[[2]]$Petal.Length))
map.values <- lapply(irisKV[3:4], "[[", 2)
v <- do.call(rbind, map.values)
tmp <- by(v, v$Species, function(x) {
   curSpecies <- as.character(x$Species[1])
   data.frame(tot=sum(x$Petal.Length), n=nrow(x))
}) 
-->

<p>The easiest way to illustrate MapReduce is through example.  Given the <code>irisRddf</code> data we just created, let&#39;s try a couple of computations:</p>

<ul>
<li>Compute the global maximum <code>Petal.Length</code></li>
<li>Compute the mean <code>Petal.Length</code> by species</li>
</ul>

<h4>Global maximum <code>Petal.Length</code></h4>

<p>Recall that <code>irisRddf</code> is a random partitioning of the <code>iris</code> data, split into 4 key-value pairs.  To compute the global maximum petal length, we simply need to compute the maximum petal length for each key-value pair in the map and then combine these maximums in the reduce and take the max of maxes.  To ensure that all of our maximum values computed in the map go to the same reduce task, we need to emit the same key each time we <code>collect()</code>.  We emit the key <code>&quot;max&quot;</code> each time.  This will ensure that even across multiple map processes, all results with emitted key <code>&quot;max&quot;</code> will be shuffled into the same reduce task, which will have <code>reduce.key = &quot;max&quot;</code>.  We write the map as follows:</p>

<pre><code class="r"># map expression to emit max petal length for each k/v pair
maxMap &lt;- expression({
   for(curMapVal in map.values)
      collect(&quot;max&quot;, max(curMapVal$Petal.Length))
})
</code></pre>

<p>The <code>map.keys</code> and <code>map.values</code> lists for the current block of input data being processed are available inside the map.  We don&#39;t care about the input keys in this case.  We step through <code>map.values</code> and emit the maximum petal length for each map value.</p>

<p>Then in the reduce, we set up the variable <code>globalMax</code> which we will update as new maximum values arrive.  In the <code>reduce</code> part of the expression, we concatenate the current value of <code>globalMax</code> to the new batch of <code>reduce.values</code> and compute the maximum of that - thus computing the maximum of maximums.  When all <code>reduce.values</code> have been processed, we call <code>collect()</code> to emit the <code>reduce.key</code> (<code>&quot;max&quot;</code>), and the computed global maximum.</p>

<pre><code class="r"># reduce expression to compute global max petal length
maxReduce &lt;- expression(
   pre={
      globalMax &lt;- NULL
   },
   reduce={
      globalMax &lt;- max(c(globalMax, unlist(reduce.values)))
   },
   post={
      collect(reduce.key, globalMax)
   }
)
</code></pre>

<p>We can execute the job with the following:</p>

<pre><code class="r"># execute the job
maxRes &lt;- mrExec(irisDdf,
   map = maxMap,
   reduce = maxReduce
)
</code></pre>

<pre><code>* Getting basic &#39;ddo&#39; attributes...
</code></pre>

<p>The output of <code>mrExec</code> is a &quot;ddo&quot; object.  Since we only output one key-value pair, and the key is <code>&quot;globalMax&quot;</code>, we can get the result with:</p>

<pre><code class="r"># look at the result
maxRes[[&quot;max&quot;]]
</code></pre>

<pre><code>[[1]]
[1] &quot;max&quot;

[[2]]
[1] 6.9
</code></pre>

<p>To go through what happened in this job in more detail, here is a visual depiction of what happened:</p>

<p><img src="image/mr1.svg" width="650px" alt="mr1" style="display:block; margin:auto"/></p>

<!-- ![mr1](image/mr1.png) -->

<p>In this diagram, we illustrate how the MapReduce would be carried out if there are two map tasks running.  The key-value pairs with keys <code>&quot;1&quot;</code> and <code>&quot;2&quot;</code> get sent to one map task, and the other two key-value pairs get sent to the other map task.  The first map has available to compute on the objects <code>map.keys = list(&quot;1&quot;, &quot;2&quot;)</code> and <code>map.values</code>, a list of the values corresponding to keys <code>&quot;1&quot;</code> and <code>&quot;2&quot;</code>.  In our map expression, we iterate through each of the two <code>map.value</code>s and emit key-value pairs shown after the map in the diagram.  This is done for both map tasks.  Then the shuffle/sort groups the data by map output key.  In this case, all map outputs have the same key, so they all get grouped together to be sent to one reduce.  If there are several reduce tasks running, in this case there will only be one doing any work, since there is only one unique map output key.  In the reduce, we have <code>reduce.key = &quot;max&quot;</code> and a list <code>reduce.values = list(6.9, 5.8, 6.7, 6.4)</code> (note that with different reduce buffer settings, it could be that we first operate on <code>reduce.values = list(6.9, 5.8)</code> and then update the result with <code>reduce.values = list(6.7, 6.4)</code>).  The reduce expression is applied to the data, and the final output is emitted, the global maximum.</p>

<p>We will look at a slightly more involved example next.  </p>

<p>First, note that there are several ways to get to the desired result.  Another way we could have written the map would be to take advantage of having several <code>map.keys</code> and <code>map.values</code> in a given running map task.  We can compute the max of the maximum of each individual subset, and then only emit one key-value pair per map task:</p>

<pre><code class="r"># another map expression to emit max petal length
maxMap2 &lt;- expression(
   collect(
      &quot;max&quot;,
      max(sapply(map.values, function(x) max(x$Petal.Length))))
)
</code></pre>

<p>With this, we are emitting less data to the reduce.  Typically intermediate data is written to disk and then read back by the reduce, so it is usually a good idea to send as little data to the reduce as possible.</p>

<h4>Mean <code>Petal.Length</code> by species</h4>

<p>Now we look at an example that shows a little more of a shuffle/sort and also illustrates how a simple summary statistic, the mean, can be broken into independent operations.</p>

<p>Suppose we would like to compute the mean petal length by species.  Computing a mean with independent operations for each subset can be done quite simply by keeping track of the sum and the length of the variable of interest in each subset, adding these up, and then dividing the final sum by the final length (note that this is not numerically stable if we are dealing with a lot of values -- see <a href="http://www.janinebennett.org/index_files/ParallelStatisticsAlgorithms.pdf">here</a> for a good reference -- these are used in the summary statistics computations for <code>updateAttributes()</code>).  </p>

<p>So computing the mean in MapReduce is easy.  But we want to compute the mean individually for each species.  We can take care of that in our map expression by breaking the data up by species, and then computing the sum and length for each and emitting them to the reduce using <code>collect()</code>.  Remember that you can call <code>collect()</code> as many times as you would like, with whatever keys and values you would like.  Here we will choose the map output keys to be the species name, to help get data to the right reduce task.</p>

<pre><code class="r"># map expression to emit sum and length of Petal.Length by species
meanMap &lt;- expression({
   v &lt;- do.call(rbind, map.values)
   tmp &lt;- by(v, v$Species, function(x) {
      collect(
         as.character(x$Species[1]),
         cbind(tot=sum(x$Petal.Length), n=nrow(x)))
   })
})
</code></pre>

<p>In this map expression, we first bind the <code>map.values</code> data frames into one data frame.  Then we call <code>by</code> to apply a function to the data frame by species, where for each subset we emit the species and the corresponding sum and length.</p>

<p>For the reduce for each unique map output key, we initialize a value <code>total = 0</code> and a length <code>nn = 0</code>.  Then, the <code>reduce</code> part of the expression is run on all incoming <code>reduce.values</code> and <code>total</code> and <code>nn</code> are updated with the new data.  When we have cycled through all <code>reduce.values</code>, we compute the mean as <code>total / nn</code> and emit the result:</p>

<pre><code class="r"># reduce to compute mean Petal.Length
meanReduce &lt;- expression(
   pre={
      total &lt;- 0
      nn &lt;- 0
   },
   reduce={
      tmp &lt;- do.call(rbind, reduce.values)
      total &lt;- total + sum(tmp[, &quot;tot&quot;])
      nn &lt;- nn + sum(tmp[, &quot;n&quot;])
   },
   post={
      collect(reduce.key, total / nn)
   }
)
</code></pre>

<p>The job is executed with:</p>

<pre><code class="r"># execute the job
meanRes &lt;- mrExec(irisRddf,
   map = meanMap,
   reduce = meanReduce
)
</code></pre>

<pre><code>* Getting basic &#39;ddo&#39; attributes...
</code></pre>

<p>And we can look at the result:</p>

<pre><code class="r"># look at the result for virginica and versicolor
meanRes[c(&quot;virginica&quot;, &quot;versicolor&quot;)]
</code></pre>

<pre><code>[[1]]
[[1]][[1]]
[1] &quot;versicolor&quot;

[[1]][[2]]
[1] 4.26


[[2]]
[[2]][[1]]
[1] &quot;virginica&quot;

[[2]][[2]]
[1] 5.552
</code></pre>

<p>And now we illustrate what happened in this job:</p>

<p><img src="image/mr2.svg" width="650px" alt="mr2" style="display:block; margin:auto"/></p>

<!-- ![mr2](image/mr2.png) -->

<p>We assume the same setup of key-value pairs being sent to two map tasks as before in the global max example.  Each map task takes its input values and <code>rbind</code>s them into a single data frame.  Then for each species subset, the species is output as the key and the sum and length are output as the value.  We see that each map task outputs data for each species.  Then the shuffle/sort takes all output with key &quot;setosa&quot; and sends it to one reduce task, etc.  Each reduce task takes its input, sums the sums and lengths, and emits a resulting mean.</p>

<p>Hopefully these examples start give an impression of the types of things that can be done with MapReduce and how it can be done in <code>datadr</code>.  </p>

<p>Remember that this MapReduce interface works on any backend, specifically RHIPE.  Those familiar with RHIPE will notice that the interface is nearly identical to that of RHIPE, but we have made some changes to make it more general.</p>

<!-- ```{r mean_map2}
map <- expression({
   for(i in seq_along(map.keys)) {
      k <- map.keys[[i]]
      v <- map.values[[i]]
      
      tmp <- by(v, v$Species, function(x) {
         curSpecies <- as.character(x$Species[1])
         collect(
            curSpecies, 
            data.frame(tot=sum(x$Petal.Length), n=nrow(x)))
      })
   }
}) -->

</div>


<div class='tab-pane' id='other-options'>
<h3>Other Options</h3>

<p>The examples we have seen have illustrated basic functionality of MapReduce in <code>datadr</code>.  There are additional options that provide fine-tuned control over some of the aspects of the MapReduce execution.</p>

<h4>The <code>setup</code> expression</h4>

<p>In addition to <code>map</code> and <code>reduce</code>, another expression that can be provided to <code>mrExec()</code> is <code>setup</code>.  This expression is executed prior to any map or reduce tasks, and is typically used to load a required library, etc.  Depending on the backend, your <code>map</code> and <code>reduce</code> expression code may be executed on multiple nodes of a cluster, and these remote R sessions need to have all of the data and packages available to do the correct computation on your data.</p>

<p>For example, suppose in the mean by species example that we wanted to use the <code>plyr</code> package to compute the mean by species inside each map task.  Then we could specify:</p>

<pre><code class="r"># example of a setup expression
setup &lt;- expression({
   suppressMessages(library(plyr))
})
</code></pre>

<p>It is a good practice to wrap calls to <code>library()</code> with <code>suppressMessages()</code> because some backends such as RHIPE interpret console output as an error.  Now we could change our map expression to something like this:</p>

<pre><code class="r"># alternative to meanMap using plyr
meanMap2 &lt;- expression({
   v &lt;- do.call(rbind, map.values)
   dlply(v, .(Species), function(x) {
      collect(
         as.character(x$Species[1]),
         cbind(tot=sum(x$Petal.Length), n=nrow(x)))
   })
})
</code></pre>

<p>We can execute it with:</p>

<pre><code class="r">meanRes &lt;- mrExec(irisRddf,
   setup = setup,
   map = meanMap2,
   reduce = meanReduce
)
</code></pre>

<pre><code>* Getting basic &#39;ddo&#39; attributes...
</code></pre>

<h4>The <code>params</code> argument</h4>

<p>If your <code>map</code> and/or <code>reduce</code> expressions rely on data in your local environment, you need to specify these in a named list as the <code>params</code> argument to <code>mrExec()</code>.  The reason for this is that the <code>map</code> and <code>reduce</code> will be executed on remote machines and any data that they rely on has to be packaged up and shipped to the nodes.  Note that when using <code>divide()</code> and <code>recombine()</code>, any functions you supply are searched to see if they reference local data objects and they are added to <code>params</code> automatically for the MapReduce calls done inside those functions, so you do not need to worry about it in those cases.</p>

<p>Suppose, for example, in our mean calculation, we want to convert the petal length measurement from centimeters to millimeters, using a conversion factor <code>cm2mm = 10</code> that is an object available in the global environment.  Of course this is a silly example because we could simply multiply the result by 10 in the reduce without passing the object, and also because we could do the conversion after reading the result back in.  More realistic cases will surely arise in your actual analyses, but for now, we use this example just to illustrate:</p>

<pre><code class="r">cm2mm &lt;- 10

meanMap3 &lt;- expression({
   v &lt;- do.call(rbind, map.values)
   dlply(v, .(Species), function(x) {
      collect(
         as.character(x$Species[1]),
         cbind(tot=sum(x$Petal.Length) * cm2mm, n=nrow(x)))
   })
})

meanRes &lt;- mrExec(irisRddf,
   setup = setup,
   map = meanMap3,
   reduce = meanReduce,
   params = list(cm2mm = cm2mm)
)
</code></pre>

<pre><code>* Getting basic &#39;ddo&#39; attributes...
</code></pre>

<h4>The <code>control</code> argument</h4>

<p>The <code>control</code> argument to <code>mrExec()</code> provides a way to specify backend-specific parameters that determine how various aspects of the backend will operate (such as number of map and reduce tasks, buffer sizes, number of cores to use, etc.).  As these depend on the backend being used, we will discuss <code>control</code> individually for each backend in the <a href="#backend-choices">Store/Compute Backends</a> section.</p>

<p>Note that the <code>control</code> argument is available in <code>divide()</code> and <code>recombine()</code> as well.</p>

<h4>The <code>output</code> argument</h4>

<p>The output argument allows you to specify where and how the output will be stored.  This is to be a &quot;kvConnection&quot; object, described in the <a href="#backend-choices">Store/Compute Backends</a> section for each implemented backend.  </p>

<p>If <code>output=NULL</code> (the default), then an attempt will be made to read the output from whatever backend the input was in to memory.  If <code>output</code> is a different storage mechanism than <code>input</code>, a conversion will be made.</p>

<h4>Distributed counters: <code>counter()</code></h4>

<p>It is possible to increment a distributed counter inside a map or reduce expression.  This can be useful for tracking things happening inside the map and reduce processes across the entire job.  Counters can be used through the function <code>counter()</code>, which is made available to be called inside any map or reduce expression.  The counter takes 3 arguments:</p>

<pre><code class="r">counter(group, name, value)
</code></pre>

<p>A call to <code>counter()</code> tells the MapReduce job to add an increment of <code>value</code> to a counter identified by its <code>group</code> and <code>name</code>.</p>

<p>For example, let&#39;s add a counter to our example job:</p>

<pre><code class="r">meanMap4 &lt;- expression({
   counter(&quot;counterTest&quot;, &quot;mapValuesProcessed&quot;, length(map.values))

   v &lt;- do.call(rbind, map.values)
   dlply(v, .(Species), function(x) {
      collect(
         as.character(x$Species[1]),
         cbind(tot=sum(x$Petal.Length) * cm2mm, n=nrow(x)))
   })
})

meanRes &lt;- mrExec(irisRddf,
   setup = setup,
   map = meanMap4,
   reduce = meanReduce,
   params = list(cm2mm = cm2mm)
)
</code></pre>

<pre><code>* Getting basic &#39;ddo&#39; attributes...
</code></pre>

<p>We added a counter to the map expression that increments the distributed counter in group <code>&quot;counterTest&quot;</code> with the name <code>&quot;mapValuesProcessed&quot;</code>.  As map tasks running in parallel are provided new data, the length of <code>map.values</code> is added to this distributed counter.  Counters are stored as an attribute of the result, and we can look at the counters with the following:</p>

<pre><code class="r">counters(meanRes)
</code></pre>

<pre><code>$counterTest
$counterTest$mapValuesProcessed
[1] 4
</code></pre>

<p>The result is what we expect -- there were 4 input key/value pairs processed by the map.</p>

<!-- #### `status()`

Mainly used for RHIPE. -->

<!-- 
The output from the map function is processed before being sent to the reduce function, grouping the key-value pairs by key. There is one reducer for each unique key passed from the map. Each group is processed by the reducer by iterating through all the values in the group. The reduce expression is composed of three parts, "pre", "reduce", and "post". For each unique key, the "pre" and "post" expressions are executed before and after the iteration through the group elements. The "pre" expression is useful for initializing variables and the "post" expression is useful for collating results or preparing them for output.
-->

</div>


<div class='tab-pane' id='all-data-computation'>
<h3>All-Data Computation</h3>

<p>While division and recombination methods focus on per-subset computation, there are times where we would like to compute statistics over the entire data set, regardless of division.  <code>datadr</code> aims to provide a set of methods for division-agnostic computations.</p>

<p>Currently, we have implemented a <code>quantile()</code> function.  We have code to do other tabulations and summary statistics which are part of <code>updateAttributes()</code>, but currently we have not exposed a nice interface for this.  This will come shortly.</p>

<p>With quantile estimation, we can obtain other related quantities of interest, such as histograms, boxplots, etc.</p>

</div>


<div class='tab-pane' id='quantiles'>
<h3>Quantiles</h3>

<p>By far the most common thing we tend to compute over the entire data other than summary statistics and tabulations is quantiles.  With <code>datadr</code>, there is a very simple interface to computing quantiles over the entire data set regardless of division.</p>

<p>To be able to compute quantiles, a &quot;ddf&quot; object must be supplied, and the <code>range</code> attribute of the variable of interest must have been computed using <code>updateAttributes()</code>.  The range is required because the quantile estimation algorithm takes the range of the variable and slices it into a grid of <code>nBins</code> bins.  Each observation of the variable is placed into the bin of the interval that it falls in and the bin counts are tabulated.  Then the resulting table is turned into a quantile estimate.</p>

<p>The quantile estimation returns results similar to that of <code>type=1</code> in R&#39;s base <code>quantile()</code> function.</p>

<h4>Example: adult data</h4>

<p>Here we provide a quick example of how to compute quantiles.  We have implemented a <code>quantile()</code> method (see <code>?quantile.ddf</code>) that at a minimum requires a &quot;ddf&quot; object and a specification of <code>var</code>, the variable you would like to compute the quantiles of.</p>

<p>We will use the <code>adult</code> data from before.  Let&#39;s load it and create a by education division:</p>

<pre><code class="r"># load adult data for quantile example
data(adult)
adultDdf &lt;- ddf(adult)
</code></pre>

<pre><code>* Getting basic &#39;ddo&#39; attributes...
* Getting basic &#39;ddf&#39; attributes...
</code></pre>

<pre><code class="r"># divide it by education
# must have update=TRUE to get range of variables
byEd &lt;- divide(adultDdf, by=&quot;education&quot;, update=TRUE)
</code></pre>

<pre><code>* Verifying parameters...
* Applying division...
* Getting basic &#39;ddo&#39; attributes...
* Running map/reduce to get missing attributes...
* Getting basic &#39;ddo&#39; attributes...
</code></pre>

<p>There&#39;s no reason to divide by education other than to illustrate that this method operates on arbitrary divisions of the data.</p>

<p>We can compute the quantiles with:</p>

<pre><code class="r"># compute quantiles of hoursperweek
hpwQuant &lt;- quantile(byEd, var=&quot;hoursperweek&quot;)
</code></pre>

<pre><code>* Getting basic &#39;ddo&#39; attributes...
</code></pre>

<pre><code class="r">hpwQuant
</code></pre>

<pre><code>          fval      q
1    0.000e+00  1.000
2    3.071e-05  1.000
3    6.142e-05  1.000
4    9.213e-05  1.000
5    1.228e-04  1.000
 [ reached getOption(&quot;max.print&quot;) -- omitted 394 rows ]
</code></pre>

<p>The result is simply a data frame of &quot;f-values&quot; <code>fval</code> and quantiles <code>q</code>.  We can plot the result with:</p>

<pre><code class="r">plot(hpwQuant)
</code></pre>

<p><img src="figure/plot_hpw_quant.png" alt="plot of chunk plot_hpw_quant"> </p>

<p>Recall the quantiles (y-axis) are hours worked in a week.  Some people work too much.</p>

<h4>Keeping all data at the tails</h4>

<p>A common thing we want to do with all-data quantile estimates is retain more observations in the tails.  With large data sets and heavy tails, it can be good to know about all of the observations located in the tails.  With the <code>quantile()</code> method for &quot;ddf&quot; objects, it is possible to specify a parameter <code>tails</code>, which you can set to a positive integer.  The <code>tails</code> argument tells the quantile method how many exact observations to keep at each side of the distribution.  These exact values are appended to the quantile estimates to provide more detail at the tails of the distribution.  The default is <code>tails=100</code>.</p>

<h4>Conditioning on variables</h4>

<p>It is possible to condition on a categorical variable when computing quantiles, so that you get a distribution per level of that categorical variable.  This can be useful when the data is very large for each category (otherwise, you can do this using <code>divide()</code> and <code>recombine()</code>).  Here is an example of the quantiles of hours worked per week by gender:</p>

<pre><code class="r"># compute quantiles of hoursperweek by sex
hpwBySexQuant &lt;- quantile(byEd, var=&quot;hoursperweek&quot;, by=&quot;sex&quot;)
</code></pre>

<pre><code>* Getting basic &#39;ddo&#39; attributes...
</code></pre>

<pre><code class="r">xyplot(q ~ fval, groups=group, data=hpwBySexQuant, auto.key=TRUE)
</code></pre>

<p><img src="figure/hpw_quant_bysex.png" alt="plot of chunk hpw_quant_bysex"> </p>

</div>


<div class='tab-pane' id='backend-choices'>
<h3>Backend Choices</h3>

<p>The examples we have seen so far have used very small datasets.  What if we have more data than fits in memory?  In this section we cover additional backends to <code>datadr</code> that allow us to scale the D&amp;R approach to very large datasets.</p>

<p><code>datadr</code> has been designed to be extensible, providing the same interface to multiple backends.  Thus all of the examples we have illustrated so far can be run with the code unchanged on data registered to a different backend.</p>

<p>The general requirements for a backend to the <code>datadr</code> interface are key-value storage and MapReduce computation.</p>

<p><img src="image/scalableenv.svg" width="450px" alt="scalableenv" style="display:block; margin:auto"/></p>

<!-- ![scalableenv](image/scalableenv.png) -->

<p>Additionally, a backend must have bindings allow us to access data and interface with MapReduce from inside of R.</p>

<p>All of the examples we have seen so far have been for &quot;small&quot; data, using in-memory R lists as the key-value store and a simple R implementation of MapReduce to provide computation.  Two other options have been implemented for &quot;medium&quot; and &quot;large&quot; data.</p>

<p><img src="image/backends4.svg" width="650px" alt="backends" style="display:block; margin:auto"/></p>

<p>We spend much of our time in RHIPE with very large datasets.  This is the only implemented backend that requires substantial effort to get up and running, which entails installing and configuring Hadoop and RHIPE on a cluster.  The other two options can be used on a single workstation.  The &quot;medium&quot; option stores data on local disk and processes it using multicore R.  This is a great intermediate backend and is particularly useful for processing results of Hadoop data that are still too large to fit into memory.  In addition to operating on small data, the &quot;small&quot; option of in-memory data works well as a backend for reading in a small subset of a larger data set and testing methods before applying across the entire data set.</p>

<p>The &quot;medium&quot; and &quot;large&quot; out-of-memory key-value storage options require a connection to be established with the backend.  Other than that, the only aspect of the interface that changes from one backend to another is a <code>control</code> method, from which the user can specify backend-specific settings and parameters.  We will provide examples of how to use these different backends in this section.</p>

<p>For each backend, we will in general follow the process of the following:</p>

<ul>
<li>Initiating a connection to the backend</li>
<li>Adding data to the connection</li>
<li>Initiating a &quot;ddo&quot; or &quot;ddf&quot; object on the connection</li>
<li>A D&amp;R example</li>
<li>A MapReduce example</li>
</ul>

</div>


<div class='tab-pane' id='small-memory--cpu'>
<h3>Small: Memory / CPU</h3>

<p>The examples we have seen so far have all been based on in-memory key-value pairs.  Thus there will be nothing new in this section.  However, we will go through the process anyway to draw comparisons to the other backends and show how the interface stays the same.</p>

<p>We will stick with a very simple example using the <code>iris</code> data.</p>

<h4>Initiating an in-memory &quot;ddf&quot;</h4>

<p>With the in-memory backend, there is not a storage backend to &quot;connect&quot; to and add data to.  We can jump straight to initializing a &quot;ddo&quot; or &quot;ddf&quot; object from data we already have in our environment.</p>

<p>For example, suppose we have the following collection of key-value pairs:</p>

<pre><code class="r">irisKV &lt;- list(
   list(&quot;key1&quot;, iris[1:40,]),
   list(&quot;key2&quot;, iris[41:110,]),
   list(&quot;key3&quot;, iris[111:150,]))
</code></pre>

<p>As we have seen before, we can initialize this as a &quot;ddf&quot; object with the following:</p>

<pre><code class="r"># initialize a &quot;ddf&quot; object from irisKV
irisDdf &lt;- ddf(irisKV)
</code></pre>

<pre><code>* Getting basic &#39;ddo&#39; attributes...
* Getting basic &#39;ddf&#39; attributes...
</code></pre>

<h4>D&amp;R example</h4>

<p>For a quick example, let&#39;s create a &quot;by species&quot; division of the data, and then do a recombination to compute the coefficients of a linear model of sepal length vs. sepal width:</p>

<pre><code class="r"># divide in-memory data by species
bySpecies &lt;- divide(irisDdf, 
   by = &quot;Species&quot;)
</code></pre>

<pre><code>* Verifying parameters...
* Applying division...
* Getting basic &#39;ddo&#39; attributes...
</code></pre>

<pre><code class="r"># compute lm coefficients for each division and rbind them
recombine(bySpecies, 
   apply = function(x) {
      coefs &lt;- coef(lm(Sepal.Length ~ Petal.Length, data=x))
      data.frame(slope=coefs[2], intercept=coefs[1])
   },
   combine = combRbind())
</code></pre>

<pre><code>* Verifying suitability of &#39;apply&#39; for division type...
* Verifying suitability of &#39;output&#39; for specified &#39;combine&#39;...
* Testing the division method on a subset...
* Applying to all subsets...
* Getting basic &#39;ddo&#39; attributes...
</code></pre>

<pre><code>     Species  slope intercept
1     setosa 0.5423     4.213
2 versicolor 0.8283     2.408
3  virginica 0.9957     1.060
</code></pre>

<h4>MapReduce example</h4>

<p>For a MapReduce example, let&#39;s take the <code>bySpecies</code> data and find the 5 records with the highest sepal width:</p>

<pre><code class="r"># map returns top 5 rows according to sepal width
top5map &lt;- expression({
   v &lt;- do.call(rbind, map.values)
   collect(&quot;top5&quot;, v[order(v$Sepal.Width, decreasing=TRUE)[1:5],])
})

# reduce collects map results and then iteratively rbinds them and returns top 5
top5reduce &lt;- expression(
   pre = {
      top5 &lt;- NULL
   }, reduce = {
      top5 &lt;- rbind(top5, do.call(rbind, reduce.values))
      top5 &lt;- top5[order(top5$Sepal.Width, decreasing=TRUE)[1:5],]
   }, post = {
      collect(reduce.key, top5)
   }
)

# execute the job
top5 &lt;- mrExec(bySpecies, map = top5map, reduce = top5reduce)
</code></pre>

<pre><code>* Getting basic &#39;ddo&#39; attributes...
</code></pre>

<pre><code class="r"># get the result
top5[[1]]
</code></pre>

<pre><code>[[1]]
[1] &quot;top5&quot;

[[2]]
   Sepal.Length Sepal.Width Petal.Length Petal.Width
16          5.7         4.4          1.5         0.4
34          5.5         4.2          1.4         0.2
33          5.2         4.1          1.5         0.1
15          5.8         4.0          1.2         0.2
6           5.4         3.9          1.7         0.4
</code></pre>

<p>Now we&#39;ll go through these same steps for the other backends.</p>

</div>


<div class='tab-pane' id='medium-disk--multicore'>
<h3>Medium: Disk / Multicore</h3>

<p>The &quot;medium&quot; key-value backend stores data on your machine&#39;s local disk, and is good for datasets that are bigger than will fit in (or are manageable in) your workstation&#39;s memory, but not so big that processing them with the available cores on your workstation becomes infeasible.  Typically this is good for data in the hundreds of megabytes.  It can be useful sometimes to store even very small datasets on local disk.</p>

<h4>Initiating a disk connection</h4>

<p>To initiate an local disk connection, we use the function <code>localDiskConn()</code>, and simply point it to a directory on our local file system.</p>

<pre><code class="r"># initiate a disk connection to a new directory /private/tmp/irisKV
irisDiskConn &lt;- localDiskConn(&quot;/private/tmp/irisKV&quot;, autoYes=TRUE)
</code></pre>

<pre><code>* Loading connection attributes
</code></pre>

<p>By default, if the directory does not exist, <code>localDiskConn()</code> will ask you if you would like to create the directory.  Since we specify <code>autoYes=TRUE</code>, the directory is automatically created.</p>

<pre><code class="r"># print the connection object
irisDiskConn
</code></pre>

<pre><code>localDiskConn connection
  loc=/private/tmp/irisKV; nBins=0
</code></pre>

<p><code>irisDiskConn</code> is simply a &quot;kvConnection&quot; object that points to the directory.  Meta data containing data attributes is also stored in this directory.  If we lose the connection object <code>irisDiskConn</code>, the data still stays on the disk, and we can get our connection back by calling</p>

<pre><code class="r">irisDiskConn &lt;- localDiskConn(&quot;/private/tmp/irisKV&quot;)
</code></pre>

<pre><code>* Loading connection attributes
</code></pre>

<p>Any meta data that was there is also read in.  If you would like to connect to a directory but reset all meta data, you can call <code>localDiskConn()</code> with <code>reset=TRUE</code>.</p>

<p>Data is stored in a local disk connection by creating a new <code>.Rdata</code> file for each key-value pair.  For data with a very large number of key-value pairs, we can end up with too many files in a directory for the file system to handle efficiently.  It is possible to specify a parameter <code>nBins</code> to <code>localDiskConn()</code>, which tells the connection that new data should be equally placed into <code>nbins</code> subdirectories.  The default is <code>nBins=0</code>.</p>

<h4>Adding data</h4>

<p>We have initiated a &quot;localDiskConn&quot; connection, but it is just an empty directory.  We need to add data to it.  With the same key-value pairs as before:</p>

<pre><code class="r">irisKV &lt;- list(
   list(&quot;key1&quot;, iris[1:40,]),
   list(&quot;key2&quot;, iris[41:110,]),
   list(&quot;key3&quot;, iris[111:150,]))
</code></pre>

<p>We can add key-value pairs to the connection with <code>addData()</code>, which takes the connection object as its first argument and a list of key-value pairs as the second argument.  For example:</p>

<pre><code class="r">addData(irisDiskConn, irisKV[1:2])
</code></pre>

<pre><code>Warning: Element 1 of input data already has data already on disk of the same key.  Set overwrite to TRUE or change key of input data.
Warning: Element 2 of input data already has data already on disk of the same key.  Set overwrite to TRUE or change key of input data.
</code></pre>

<p>Here we added the first 2 key-value pairs to disk.  We can verify this by looking in the directory:</p>

<pre><code class="r">list.files(irisDiskConn$loc)
</code></pre>

<pre><code>[1] &quot;_meta&quot;                                 
[2] &quot;07abaecdababc84098369b43ae953523.Rdata&quot;
[3] &quot;ac3c8cadb82078a1a3de9d0d59822e4d.Rdata&quot;
[4] &quot;b1f1dba2f126bc3208b6b84121503757.Rdata&quot;
</code></pre>

<p><code>&quot;_meta&quot;</code> is a directory where the connection metadata is stored.  The two <code>.Rdata</code> files are the two key-value pairs we just added.  The file name is determined by the md5 hash of the data in the key (and we don&#39;t have to worry about this).</p>

<p>We can call <code>addData()</code> as many times as we would like to continue to add data to the directory.  Let&#39;s add the final key-value pair:</p>

<pre><code class="r">addData(irisDiskConn, irisKV[3])
</code></pre>

<pre><code>Warning: Element 1 of input data already has data already on disk of the
same key.  Set overwrite to TRUE or change key of input data.
</code></pre>

<p>Now we have a connection with all of the data in it.</p>

<h4>Initializing a &quot;ddf&quot; object</h4>

<p>We can initialize a &quot;ddo&quot; or &quot;ddf&quot; object with our disk connection object:</p>

<pre><code class="r"># initialize a &quot;ddf&quot; object from irisDiskConn
irisDdf &lt;- ddf(irisDiskConn)
</code></pre>

<pre><code>* Reading in existing &#39;ddo&#39; attributes
* Reading in existing &#39;ddf&#39; attributes
</code></pre>

<p>As noted before, with in-memory data, we initialize &quot;ddo&quot; or &quot;ddf&quot; objects with in-memory key-value pairs.  For all other backends, we pass a connection object.  <code>irisDdf</code> is now a distributed data frame that behaves in the same way as the one we created for the in-memory case.  The data itself though is located on disk.  </p>

<p>The connection object is saved as an attribute of the &quot;ddo/ddf&quot; object.</p>

<pre><code class="r"># print irisDdf
irisDdf
</code></pre>

<pre><code>
Distributed data object of class &#39;kvLocalDisk&#39; with attributes: 

&#39;ddo&#39; attribute | value
----------------+-----------------------------------------------------------
 keys           | keys are available through getKeys(dat)
 totSize        | 1.95 KB
 nDiv           | 3
 splitSizeDistn | use splitSizeDistn(dat) to get distribution
 example        | use kvExample(dat) to get an example subset
 bsvInfo        | [empty] no BSVs have been specified

&#39;ddf&#39; attribute | value
----------------+-----------------------------------------------------------
 vars           | Sepal.Length(num), Sepal.Width(num), and 3 more
 transFn        | identity (original data is a data frame)
 nRow           | 150
 splitRowDistn  | use splitRowDistn(dat) to get distribution
 summary        | use summary(dat) to see summaries

localDiskConn connection
  loc=/private/tmp/irisKV; nBins=0
</code></pre>

<p>We see that the connection info for the object is added to the printout of <code>irisDdf</code>.  Also, note that nearly all of the attributes have not been populated, including the keys.  This is because the data is on disk and we need to pass over it to compute most of the attributes:</p>

<pre><code class="r"># update irisDdf attributes
irisDdf &lt;- updateAttributes(irisDdf)
</code></pre>

<pre><code>All (implemented) attributes have already been computed.
</code></pre>

<h4>D&amp;R Example</h4>

<p>Let&#39;s see how the code looks for the D&amp;R example on the local disk data:</p>

<pre><code class="r"># divide local disk data by species
bySpecies &lt;- divide(irisDdf, 
   by = &quot;Species&quot;,
   output = localDiskConn(&quot;/private/tmp/bySpecies&quot;, autoYes=TRUE),
   update = TRUE)
</code></pre>

<pre><code>* Verifying parameters...
* Applying division...
* Loading connection attributes
* Reading in existing &#39;ddo&#39; attributes
All (implemented) attributes have already been computed.
</code></pre>

<p>This code is the same as what we used for the in-memory data except that in <code>divide()</code>, we also need to specify an output connection.  If <code>output</code> is not provided, an attempt is made to read the data in to an in-memory connection.  Here we specify that we would like the output of the division to be stored on local disk in <code>&quot;/private/tmp/bySpecies&quot;</code>.  </p>

<p>As stated before, note that local disk objects persists on disk.  I know where the data and metadata for the <code>bySpecies</code> object is located.  If I lose my R session or remove my object, I can get it back.  All attributes are stored as meta data at the connection, so that I don&#39;t need to worry about recomputing anything:</p>

<pre><code class="r"># remove the R object &quot;bySpecies&quot;
rm(bySpecies)
# now reinitialize
bySpecies &lt;- ddf(localDiskConn(&quot;/private/tmp/bySpecies&quot;))
</code></pre>

<pre><code>* Loading connection attributes
* Reading in existing &#39;ddo&#39; attributes
* Reading in existing &#39;ddf&#39; attributes
</code></pre>

<p>The code for the recombination remains exactly the same:</p>

<pre><code class="r"># compute lm coefficients for each division and rbind them
recombine(bySpecies, 
   apply = function(x) {
      coefs &lt;- coef(lm(Sepal.Length ~ Petal.Length, data=x))
      data.frame(slope=coefs[2], intercept=coefs[1])
   },
   combine = combRbind())
</code></pre>

<pre><code>* Verifying suitability of &#39;apply&#39; for division type...
* Verifying suitability of &#39;output&#39; for specified &#39;combine&#39;...
* Testing the division method on a subset...
* Applying to all subsets...
* Getting basic &#39;ddo&#39; attributes...
</code></pre>

<pre><code>     Species  slope intercept
1     setosa 0.5423     4.213
2 versicolor 0.8283     2.408
3  virginica 0.9957     1.060
</code></pre>

<h4>Interacting with local disk &quot;ddo/ddf&quot; objects</h4>

<p>Note that all interactions with local disk &quot;ddo/ddf&quot; objects are the same as those we have seen so far.  </p>

<p>For example, I can access data by index or by key:</p>

<pre><code class="r">bySpecies[[1]]
</code></pre>

<pre><code>[[1]]
[1] &quot;Species=setosa&quot;

[[2]]
   Sepal.Length Sepal.Width Petal.Length Petal.Width
1           5.1         3.5          1.4         0.2
2           4.9         3.0          1.4         0.2
 [ reached getOption(&quot;max.print&quot;) -- omitted 48 rows ]
</code></pre>

<pre><code class="r">bySpecies[[&quot;Species=setosa&quot;]]
</code></pre>

<pre><code>[[1]]
[1] &quot;Species=setosa&quot;

[[2]]
   Sepal.Length Sepal.Width Petal.Length Petal.Width
1           5.1         3.5          1.4         0.2
2           4.9         3.0          1.4         0.2
 [ reached getOption(&quot;max.print&quot;) -- omitted 48 rows ]
</code></pre>

<p>These extractors find the appropriate key-value pair files on disk, read them in, and return them.</p>

<p>Also, all the accessors like <code>getKeys()</code> work just the same:</p>

<pre><code class="r">getKeys(bySpecies)
</code></pre>

<pre><code>4a7a45a288b320d14537ba28cfdb8db5 88f9dd48ca10366898776cdf8fdc5418 
                &quot;Species=setosa&quot;             &quot;Species=versicolor&quot; 
bc1f636f7428e8dc45d7d65ee4c78a45 
             &quot;Species=virginica&quot; 
</code></pre>

<h4>MapReduce example</h4>

<p>Here we again find the top 5 <code>iris</code> records according to sepal width.</p>

<pre><code class="r"># map returns top 5 rows according to sepal width
top5map &lt;- expression({
   counter(&quot;map&quot;, &quot;mapTasks&quot;, 1)
   v &lt;- do.call(rbind, map.values)
   collect(&quot;top5&quot;, v[order(v$Sepal.Width, decreasing=TRUE)[1:5],])
})

# reduce collects map results and then iteratively rbinds them and returns top 5
top5reduce &lt;- expression(
   pre = {
      top5 &lt;- NULL
   }, reduce = {
      top5 &lt;- rbind(top5, do.call(rbind, reduce.values))
      top5 &lt;- top5[order(top5$Sepal.Width, decreasing=TRUE)[1:5],]
   }, post = {
      collect(reduce.key, top5)
   }
)

# execute the job
top5 &lt;- mrExec(bySpecies, map = top5map, reduce = top5reduce)
</code></pre>

<pre><code>* Getting basic &#39;ddo&#39; attributes...
</code></pre>

<pre><code class="r"># get the result
top5[[1]]
</code></pre>

<pre><code>[[1]]
[1] &quot;top5&quot;

[[2]]
   Sepal.Length Sepal.Width Petal.Length Petal.Width
16          5.7         4.4          1.5         0.4
34          5.5         4.2          1.4         0.2
33          5.2         4.1          1.5         0.1
15          5.8         4.0          1.2         0.2
6           5.4         3.9          1.7         0.4
</code></pre>

<p>I added the line with the call to <code>counter()</code> to the map expression to illustrate some of the control parameters described at the end of this section.</p>

<h4><code>control</code> options</h4>

<p>There are various aspects of backends that we want to be able to have control oer.  The <code>control</code> argument of a MapReduce job provides a general interface to do this.  A <code>control</code> argument is simply a named list of settings for various control parameters.</p>

<p>The following functions all run MapReduce jobs and therefore have a <code>control</code> argument:</p>

<ul>
<li><code>mrExec()</code></li>
<li><code>divide()</code></li>
<li><code>recombine()</code></li>
<li><code>updateAttributes()</code></li>
<li><code>quantile.ddf()</code></li>
</ul>

<p>Currently, the available control parameters for MapReduce on a local disk connection are:</p>

<ul>
<li><code>cluster</code>: a cluster object from <code>makeCluster()</code> to use to do distributed computation -- default is <code>NULL</code> (single core)</li>
<li><code>mapred_temp_dir</code>: where to put intermediate key-value pairs in between map and reduce -- default is <code>tempdir()</code></li>
<li><code>map_buff_size_bytes</code>: the size of batches of key-value pairs to be passed to the map -- default is 10485760 (10 Mb)</li>
<li><code>map_temp_buff_size_bytes</code>: the size of the batches of key-value pairs to flush to intermediate storage from the map output -- default is 10485760 (10 Mb)</li>
<li><code>reduce_buff_size_bytes</code>: the size of the batches of key-value pairs to send to the reduce -- default is 10485760 (10 Mb)</li>
</ul>

<p>The function <code>localDiskControl()</code> is used to create the default list.  Any parameter specified will override the default.</p>

<p>To illustrate the use of <code>control</code> for local disk connections, let&#39;s rerun the &quot;top 5&quot; MapReduce job but this time with a 3-core cluster and with a much smaller buffer size for the map input:</p>

<pre><code class="r"># create a 3 core cluster
library(parallel)
cl &lt;- makeCluster(3)

# run MapReduce job with custom control
top5a &lt;- mrExec(bySpecies, 
   map = top5map, reduce = top5reduce,
   control = localDiskControl(cluster = cl, map_buff_size_bytes = 10))
</code></pre>

<pre><code>* Getting basic &#39;ddo&#39; attributes...
</code></pre>

<p>The map and reduce tasks for this job were run on a 3-core cluster.  We set <code>map_buff_size_bytes</code> to 10.  This means that when sending key-value pairs to the map tasks, input key-value pairs will be bundled into batches of key-value pairs (minimum of 1 key-value pair per batch) that don&#39;t exceed 10 bytes.  Since our data is very small, this very low limit ensures that there is only one key-value pair per batch, and therefore that the 3 map tasks will all be doing work.  With the default buffer size of 10 Mb, all 3 input key-value pairs are sent to one map task, and in that case, it is pointless to run a multicore job because only one map task will be run.</p>

<p>We can verify that our new computation did indeed run 3 map tasks by comparing the counters from the first and second jobs:</p>

<pre><code class="r"># how many map tasks were there before setting map_buff_size_bytes
counters(top5)$map$mapTasks
</code></pre>

<pre><code>[1] 1
</code></pre>

<pre><code class="r"># how many map tasks were there after setting map_buff_size_bytes
counters(top5a)$map$mapTasks
</code></pre>

<pre><code>[1] 3
</code></pre>

</div>


<div class='tab-pane' id='large-hdfs--rhipe'>
<h3>Large: HDFS / RHIPE</h3>

<p>Very large data sets can be stored on the Hadoop Distributed File System (HDFS).  For this to work, your workstation must be connected to a Hadoop cluster with RHIPE installed.  Hadoop setup and configuration can be quite difficult, and is not discussed here.  We have used HDFS / RHIPE to store and perform D&amp;R tasks on multi-terabyte data sets.</p>

<h4>HDFS operations with RHIPE</h4>

<p>Getting ready for dealing with data in Hadoop can require some Hadoop file system operations.  Here is a quick crash course on the available functions for interacting with HDFS from R using RHIPE.</p>

<p>First we need to load and initialize RHIPE:</p>

<pre><code class="r">library(Rhipe)
</code></pre>

<pre><code>------------------------------------------------
| Please call rhinit() else RHIPE will not run |
------------------------------------------------
</code></pre>

<pre><code class="r">rhinit()
</code></pre>

<pre><code>Rhipe: Using Rhipe.jar file
Initializing Rhipe v0.73
Initializing mapfile caches
</code></pre>

<p>Now for some of the available commands:</p>

<pre><code class="r"># list files in the base directory of HDFS
rhls(&quot;/&quot;)
</code></pre>

<pre><code>  permission   owner      group size          modtime  file
1 drwxr-xr-x hafe647 supergroup    0 2013-09-25 14:57 /test
2 drwxr-xr-x hafe647 supergroup    0 2013-11-09 11:49  /tmp
3 drwxr-xr-x hafe647 supergroup    0 2013-09-13 13:12 /user
</code></pre>

<pre><code class="r"># make a directory /tmp/testfile
rhmkdir(&quot;/tmp/testfile&quot;)
</code></pre>

<pre><code>[1] TRUE
</code></pre>

<pre><code class="r"># write a couple of key-value pairs to /tmp/testfile/1
rhwrite(list(list(1, 1), list(2, 2)), file=&quot;/tmp/testfile/1&quot;)
</code></pre>

<pre><code>Wrote 0.39 KB,2 chunks, and 2 elements (100% complete)
</code></pre>

<pre><code class="r"># read those values back in
a &lt;- rhread(&quot;/tmp/testfile/1&quot;)
</code></pre>

<pre><code>Read 2 objects(0.08 KB) in 0.03 seconds
</code></pre>

<pre><code class="r"># create an R object and save a .Rdata file containing it to HDFS
d &lt;- rnorm(10)
rhsave(d, file=&quot;/tmp/testfile/d.Rdata&quot;)
# load that object back into the session
rhload(&quot;/tmp/testfile/d.Rdata&quot;)
# list the files in /tmp/testfile
rhls(&quot;/tmp/testfile&quot;)
</code></pre>

<pre><code>  permission   owner      group        size          modtime
1 drwxr-xr-x hafe647 supergroup           0 2013-11-09 11:54
2 -rw-r--r-- hafe647 supergroup   144 bytes 2013-11-09 11:54
                   file
1       /tmp/testfile/1
2 /tmp/testfile/d.Rdata
</code></pre>

<pre><code class="r"># set the HDFS working directory (like R&#39;s setwd())
hdfs.setwd(&quot;/tmp/testfile&quot;)
# now commands like rhls() go on paths relative to the HDFS working directory
rhls()
</code></pre>

<pre><code>  permission   owner      group        size          modtime
1 drwxr-xr-x hafe647 supergroup           0 2013-11-09 11:54
2 -rw-r--r-- hafe647 supergroup   144 bytes 2013-11-09 11:54
                   file
1       /tmp/testfile/1
2 /tmp/testfile/d.Rdata
</code></pre>

<pre><code class="r"># change permissions of /tmp/testfile/1
rhchmod(&quot;1&quot;, 777)
# see how permissions chagned
rhls()
</code></pre>

<pre><code>  permission   owner      group        size          modtime
1 drwxrwxrwx hafe647 supergroup           0 2013-11-09 11:54
2 -rw-r--r-- hafe647 supergroup   144 bytes 2013-11-09 11:54
                   file
1       /tmp/testfile/1
2 /tmp/testfile/d.Rdata
</code></pre>

<pre><code class="r"># delete everything we just did
rhdel(&quot;/tmp/testfile&quot;)
</code></pre>

<p>Also see <code>rhcp()</code> and <code>rhmv()</code>.</p>

<h4>Initiating a HDFS connection</h4>

<p>To initiate a connection to data on HDFS, we use the function <code>hdfsConn()</code>, and simply point it to a directory on HDFS.</p>

<pre><code class="r"># initiate an HDFS connection to a new HDFS directory /tmp/irisKV
irisHDFSconn &lt;- hdfsConn(&quot;/tmp/irisKV&quot;, autoYes=TRUE)
</code></pre>

<pre><code>* Attempting to create directory... success
* Saving connection attributes
* To initialize the data in this directory as a distributed data object or data frame, call ddo() or ddf()
</code></pre>

<p>Similar to local disk connections, by default, if the HDFS directory does not exist, <code>hdfsConn()</code> will ask you if you would like to create the directory.  Since we specify <code>autoYes=TRUE</code>, the directory is automatically created.  Also, as with local disk connections, <code>irisHDFSconn</code> is simply a &quot;kvConnection&quot; object that points to the HDFS directory which contains or will contain data, and where meta data is stored for the connection.</p>

<pre><code class="r"># print the connection object
irisHDFSconn
</code></pre>

<pre><code>hdfsConn connection
  loc=/tmp/irisKV; type=sequence
</code></pre>

<p>This simply prints the location of the HDFS directory we are connected to and the type of data it will expect.  <code>&quot;sequence&quot;</code> is the default, which is a Hadoop sequence file.  Other options are <code>&quot;map&quot;</code> and <code>&quot;text&quot;</code>.  These can be specified using the <code>type</code> argument to <code>hdfsConn()</code>.  See <code>?hdfsConn</code> for more details.</p>

<h4>Adding data</h4>

<p>There is a method <code>addData()</code> available for &quot;hdfsConn&quot; connections, but it is not recommended to use this.  The reason is that for each call of <code>addData()</code>, a new file is created on HDFS in the subdirectory that your connection points to.  If you have a lot of data, chances are that you will be adding a lot of individual files.  Hadoop does not like to handle large numbers of files.  If the data is very large, it likes a very small number of very large files.  Having a large number of files slows down job initialization and also requires more map tasks to run than would probably be desired.  However, the method is still available if you would like to use it.  Just note that the typical approach is to begin with data that is already on HDFS in some form (we will cover an example of beginning with text files on HDFS later).</p>

<p>To mimic what was done with the &quot;localDiskConn&quot; example:</p>

<pre><code class="r">irisKV &lt;- list(
   list(&quot;key1&quot;, iris[1:40,]),
   list(&quot;key2&quot;, iris[41:110,]),
   list(&quot;key3&quot;, iris[111:150,]))

addData(irisHDFSconn, irisKV)
</code></pre>

<pre><code>Wrote 11.22 KB,3 chunks, and 3 elements (100% complete)
</code></pre>

<h4>Initializing a &quot;ddf&quot; object</h4>

<p>We can initialize a &quot;ddo&quot; or &quot;ddf&quot; object by passing the HDFS connection object to <code>ddo()</code> or <code>ddf()</code>.</p>

<pre><code class="r"># initialize a &quot;ddf&quot; object from hdfsConn
irisDdf &lt;- ddf(irisHDFSconn)
</code></pre>

<pre><code>* Getting basic &#39;ddo&#39; attributes...
Read 1 objects(1.56 KB) in 0.02 seconds
* Getting basic &#39;ddf&#39; attributes...
</code></pre>

<pre><code class="r">irisDdf
</code></pre>

<pre><code>
Distributed data object of class &#39;kvHDFS&#39; with attributes: 

&#39;ddo&#39; attribute | value
----------------+-----------------------------------------------------------
 keys           | [empty] call updateAttributes(dat) to get this value
 totSize        | 5.92 KB
 nDiv           | [empty] call updateAttributes(dat) to get this value
 splitSizeDistn | [empty] call updateAttributes(dat) to get this value
 example        | use kvExample(dat) to get an example subset
 bsvInfo        | [empty] no BSVs have been specified

&#39;ddf&#39; attribute | value
----------------+-----------------------------------------------------------
 vars           | Sepal.Length(num), Sepal.Width(num), and 3 more
 transFn        | identity (original data is a data frame)
 nRow           | [empty] call updateAttributes(dat) to get this value
 splitRowDistn  | [empty] call updateAttributes(dat) to get this value
 summary        | [empty] call updateAttributes(dat) to get this value

hdfsConn connection
  loc=/tmp/irisKV; type=sequence
</code></pre>

<p>As with the disk connection <code>irisDdf</code> object, nearly all of the attributes have not been populated.</p>

<pre><code class="r"># update irisDdf attributes
irisDdf &lt;- updateAttributes(irisDdf)
</code></pre>

<pre><code>* Running map/reduce to get missing attributes...
* Attempting to create directory... success
* Saving connection attributes
* To initialize the data in this directory as a distributed data object or data frame, call ddo() or ddf()
Loading required package: codetools
Saving 3 paramaters to /tmp/rhipe-temp-params-a91f11002e0d77ffbfba779b8f9ef6f6 (use rhclean to delete all temp files)
</code></pre>

<pre><code>[Sat Nov  9 11:54:20 2013] Name:2013-11-09 11:54:20 Job: job_201311091036_0023  State: PREP Duration: 0.141
URL: http://localhost:50030/jobdetails.jsp?jobid=job_201311091036_0023
       pct numtasks pending running complete killed failed_attempts
map      0        0       0       0        0      0               0
reduce   0        0       0       0        0      0               0
       killed_attempts
map                  0
reduce               0
Waiting 5 seconds
[Sat Nov  9 11:54:25 2013] Name:2013-11-09 11:54:20 Job: job_201311091036_0023  State: RUNNING Duration: 5.205
URL: http://localhost:50030/jobdetails.jsp?jobid=job_201311091036_0023
       pct numtasks pending running complete killed failed_attempts
map      1        1       0       0        1      0               0
reduce   0        1       0       1        0      0               0
       killed_attempts
map                  0
reduce               0
Waiting 5 seconds
[Sat Nov  9 11:54:30 2013] Name:2013-11-09 11:54:20 Job: job_201311091036_0023  State: RUNNING Duration: 10.242
URL: http://localhost:50030/jobdetails.jsp?jobid=job_201311091036_0023
       pct numtasks pending running complete killed failed_attempts
map      1        1       0       0        1      0               0
reduce   0        1       0       1        0      0               0
       killed_attempts
map                  0
reduce               0
Waiting 5 seconds
[Sat Nov  9 11:54:35 2013] Name:2013-11-09 11:54:20 Job: job_201311091036_0023  State: RUNNING Duration: 15.275
URL: http://localhost:50030/jobdetails.jsp?jobid=job_201311091036_0023
       pct numtasks pending running complete killed failed_attempts
map      1        1       0       0        1      0               0
reduce   1        1       0       0        1      0               0
       killed_attempts
map                  0
reduce               0
Waiting 5 seconds
</code></pre>

<pre><code>* Getting basic &#39;ddo&#39; attributes...
Read 1 objects(0.05 KB) in 0.02 seconds
Read 10 objects(4.43 KB) in 0.02 seconds
</code></pre>

<h4>D&amp;R Example</h4>

<p>Let&#39;s see how the code looks for the D&amp;R example on the HDFS data:</p>

<pre><code class="r"># divide HDFS data by species
bySpecies &lt;- divide(irisDdf, 
   by = &quot;Species&quot;, 
   output = hdfsConn(&quot;/tmp/bySpecies&quot;, autoYes=TRUE),
   update = TRUE)
</code></pre>

<pre><code>* Verifying parameters...
* Applying division...
* Attempting to create directory... success
* Saving connection attributes
* To initialize the data in this directory as a distributed data object or data frame, call ddo() or ddf()
Saving 9 paramaters to /tmp/rhipe-temp-params-346ea712173ab721283a06f121fcfa29 (use rhclean to delete all temp files)
</code></pre>

<pre><code>[Sat Nov  9 11:54:41 2013] Name:2013-11-09 11:54:41 Job: job_201311091036_0024  State: PREP Duration: 0.082
URL: http://localhost:50030/jobdetails.jsp?jobid=job_201311091036_0024
       pct numtasks pending running complete killed failed_attempts
map      0        0       0       0        0      0               0
reduce   0        0       0       0        0      0               0
       killed_attempts
map                  0
reduce               0
Waiting 5 seconds
[Sat Nov  9 11:54:46 2013] Name:2013-11-09 11:54:41 Job: job_201311091036_0024  State: RUNNING Duration: 5.126
URL: http://localhost:50030/jobdetails.jsp?jobid=job_201311091036_0024
       pct numtasks pending running complete killed failed_attempts
map      1        1       0       0        1      0               0
reduce   0        1       0       1        0      0               0
       killed_attempts
map                  0
reduce               0
Waiting 5 seconds
[Sat Nov  9 11:54:51 2013] Name:2013-11-09 11:54:41 Job: job_201311091036_0024  State: RUNNING Duration: 10.168
URL: http://localhost:50030/jobdetails.jsp?jobid=job_201311091036_0024
       pct numtasks pending running complete killed failed_attempts
map      1        1       0       0        1      0               0
reduce   0        1       0       1        0      0               0
       killed_attempts
map                  0
reduce               0
Waiting 5 seconds
</code></pre>

<pre><code>* Getting basic &#39;ddo&#39; attributes...
Read 1 objects(1.85 KB) in 0.02 seconds
* Running map/reduce to get missing attributes...
* Attempting to create directory... success
* Saving connection attributes
* To initialize the data in this directory as a distributed data object or data frame, call ddo() or ddf()
Saving 3 paramaters to /tmp/rhipe-temp-params-3ee5dfd11ee0b4672ab3a64f3eb70d0a (use rhclean to delete all temp files)
</code></pre>

<pre><code>[Sat Nov  9 11:54:57 2013] Name:2013-11-09 11:54:57 Job: job_201311091036_0025  State: PREP Duration: 0.077
URL: http://localhost:50030/jobdetails.jsp?jobid=job_201311091036_0025
       pct numtasks pending running complete killed failed_attempts
map      0        0       0       0        0      0               0
reduce   0        0       0       0        0      0               0
       killed_attempts
map                  0
reduce               0
Waiting 5 seconds
[Sat Nov  9 11:55:02 2013] Name:2013-11-09 11:54:57 Job: job_201311091036_0025  State: RUNNING Duration: 5.136
URL: http://localhost:50030/jobdetails.jsp?jobid=job_201311091036_0025
       pct numtasks pending running complete killed failed_attempts
map      1        1       0       0        1      0               0
reduce   0        1       0       1        0      0               0
       killed_attempts
map                  0
reduce               0
Waiting 5 seconds
[Sat Nov  9 11:55:07 2013] Name:2013-11-09 11:54:57 Job: job_201311091036_0025  State: RUNNING Duration: 10.185
URL: http://localhost:50030/jobdetails.jsp?jobid=job_201311091036_0025
       pct numtasks pending running complete killed failed_attempts
map      1        1       0       0        1      0               0
reduce   0        1       0       1        0      0               0
       killed_attempts
map                  0
reduce               0
Waiting 5 seconds
[Sat Nov  9 11:55:12 2013] Name:2013-11-09 11:54:57 Job: job_201311091036_0025  State: RUNNING Duration: 15.214
URL: http://localhost:50030/jobdetails.jsp?jobid=job_201311091036_0025
       pct numtasks pending running complete killed failed_attempts
map      1        1       0       0        1      0               0
reduce   1        1       0       0        1      0               0
       killed_attempts
map                  0
reduce               0
Waiting 5 seconds
</code></pre>

<pre><code>* Getting basic &#39;ddo&#39; attributes...
Read 1 objects(0.09 KB) in 0.02 seconds
Read 9 objects(4.22 KB) in 0.02 seconds
</code></pre>

<p>As with the local disk data, we specify an HDFS output connection, indicating to store the results of the division to <code>&quot;/tmp/bySpecies&quot;</code> on HDFS.  As with local disk data, this object and all meta data persists on disk.  </p>

<p>If we were to leave our R session and want to reinstate our <code>bySpecies</code> object in a new session:</p>

<pre><code class="r"># reinitialize &quot;bySpecies&quot; by connecting to its path on HDFS
bySpecies &lt;- ddf(hdfsConn(&quot;/tmp/bySpecies&quot;))
</code></pre>

<pre><code>* Loading connection attributes
* Reading in existing &#39;ddo&#39; attributes
* Reading in existing &#39;ddf&#39; attributes
</code></pre>

<p>The code for the recombination remains exactly the same:</p>

<pre><code class="r"># compute lm coefficients for each division and rbind them
recombine(bySpecies, 
   apply = function(x) {
      coefs &lt;- coef(lm(Sepal.Length ~ Petal.Length, data=x))
      data.frame(slope=coefs[2], intercept=coefs[1])
   },
   combine = combRbind())
</code></pre>

<pre><code>* Verifying suitability of &#39;apply&#39; for division type...
* Verifying suitability of &#39;output&#39; for specified &#39;combine&#39;...
* Testing the division method on a subset...
* Applying to all subsets...
* Attempting to create directory... success
* Saving connection attributes
* To initialize the data in this directory as a distributed data object or data frame, call ddo() or ddf()
Saving 2 paramaters to /tmp/rhipe-temp-params-607ff5c939910e022db0067094a168c1 (use rhclean to delete all temp files)
</code></pre>

<pre><code>[Sat Nov  9 11:55:19 2013] Name:2013-11-09 11:55:19 Job: job_201311091036_0026  State: PREP Duration: 0.07
URL: http://localhost:50030/jobdetails.jsp?jobid=job_201311091036_0026
       pct numtasks pending running complete killed failed_attempts
map      0        0       0       0        0      0               0
reduce   0        0       0       0        0      0               0
       killed_attempts
map                  0
reduce               0
Waiting 5 seconds
[Sat Nov  9 11:55:24 2013] Name:2013-11-09 11:55:19 Job: job_201311091036_0026  State: RUNNING Duration: 5.112
URL: http://localhost:50030/jobdetails.jsp?jobid=job_201311091036_0026
       pct numtasks pending running complete killed failed_attempts
map      0        1       0       1        0      0               0
reduce   0        1       1       0        0      0               0
       killed_attempts
map                  0
reduce               0
Waiting 5 seconds
[Sat Nov  9 11:55:30 2013] Name:2013-11-09 11:55:19 Job: job_201311091036_0026  State: RUNNING Duration: 10.302
URL: http://localhost:50030/jobdetails.jsp?jobid=job_201311091036_0026
       pct numtasks pending running complete killed failed_attempts
map      1        1       0       0        1      0               0
reduce   0        1       0       1        0      0               0
       killed_attempts
map                  0
reduce               0
Waiting 5 seconds
[Sat Nov  9 11:55:35 2013] Name:2013-11-09 11:55:19 Job: job_201311091036_0026  State: RUNNING Duration: 15.332
URL: http://localhost:50030/jobdetails.jsp?jobid=job_201311091036_0026
       pct numtasks pending running complete killed failed_attempts
map      1        1       0       0        1      0               0
reduce   1        1       0       0        1      0               0
       killed_attempts
map                  0
reduce               0
Waiting 5 seconds
</code></pre>

<pre><code>* Getting basic &#39;ddo&#39; attributes...
Read 1 objects(0.22 KB) in 0.02 seconds
Read 1 objects(0.22 KB) in 0.02 seconds
</code></pre>

<pre><code>     Species  slope intercept
1     setosa 0.5423     4.213
2  virginica 0.9957     1.060
3 versicolor 0.8283     2.408
</code></pre>

<h4>Interacting with HDFS &quot;ddo/ddf&quot; objects</h4>

<p>All interactions with HDFS &quot;ddo/ddf&quot; objects are still the same as those we have seen so far.  </p>

<pre><code class="r">bySpecies[[1]]
</code></pre>

<pre><code>Read 1 objects(1.85 KB) in 0.02 seconds
</code></pre>

<pre><code>[[1]]
[1] &quot;Species=setosa&quot;

[[2]]
   Sepal.Length Sepal.Width Petal.Length Petal.Width
1           5.1         3.5          1.4         0.2
2           4.9         3.0          1.4         0.2
 [ reached getOption(&quot;max.print&quot;) -- omitted 48 rows ]
</code></pre>

<pre><code class="r">bySpecies[[&quot;Species=setosa&quot;]]
</code></pre>

<pre><code>   Sepal.Length Sepal.Width Petal.Length Petal.Width
1           5.1         3.5          1.4         0.2
2           4.9         3.0          1.4         0.2
 [ reached getOption(&quot;max.print&quot;) -- omitted 48 rows ]
</code></pre>

<p>However, there are a few caveats about extractors for these objects.  If you specify a numeric index, <code>i</code>, the extractor method returns the key-value pair for the <code>i</code>th key, as available from <code>getKeys()</code>.  Thus, if you don&#39;t have your object keys read in, you can&#39;t access data in this way.  Another important thing to keep in mind is that retrieving data by key for data on HDFS requires that the data is in a Hadoop <em>mapfile</em>.  </p>

<h4>Hadoop mapfiles</h4>

<p>Random access by key for <code>datadr</code> data objects stored on Hadoop requires that they are stored in a valid mapfile.  By default, the result of any <code>divide()</code> operation returns a mapfile.  The user need not worry about the details of this -- if operations that require the data to be a valid mapfile are not given a mapfile, they will complain and tell you to convert your data to a mapfile.  </p>

<p>For example, recall from our original data object, <code>irisDdf</code>, that the connection stated that the file type was a <em>sequence</em> file.  Let&#39;s try to retrieve the subset with key &quot;key1&quot;:</p>

<pre><code class="r">irisDdf[[&quot;key1&quot;]]
</code></pre>

<pre><code>Error: This data must not be a valid mapfile -- cannot extract subsets by
key.  Call makeExtractable() on this data.
</code></pre>

<p>We have been told to call <code>makeExtractable()</code> on this data to make subsets extractable by key.</p>

<pre><code class="r"># make data into a mapfile
irisDdf &lt;- makeExtractable(irisDdf)
</code></pre>

<pre><code>Saving 1 paramater to /tmp/rhipe-temp-params-95f69974b843850dedb4db9da095825f (use rhclean to delete all temp files)
</code></pre>

<pre><code>[Sat Nov  9 11:55:42 2013] Name:2013-11-09 11:55:42 Job: job_201311091036_0027  State: PREP Duration: 0.07
URL: http://localhost:50030/jobdetails.jsp?jobid=job_201311091036_0027
       pct numtasks pending running complete killed failed_attempts
map      0        0       0       0        0      0               0
       killed_attempts
map                  0
 [ reached getOption(&quot;max.print&quot;) -- omitted 1 row ]
Waiting 5 seconds
[Sat Nov  9 11:55:47 2013] Name:2013-11-09 11:55:42 Job: job_201311091036_0027  State: RUNNING Duration: 5.105
URL: http://localhost:50030/jobdetails.jsp?jobid=job_201311091036_0027
       pct numtasks pending running complete killed failed_attempts
map      1        1       0       0        1      0               0
       killed_attempts
map                  0
 [ reached getOption(&quot;max.print&quot;) -- omitted 1 row ]
Waiting 5 seconds
[Sat Nov  9 11:55:52 2013] Name:2013-11-09 11:55:42 Job: job_201311091036_0027  State: RUNNING Duration: 10.139
URL: http://localhost:50030/jobdetails.jsp?jobid=job_201311091036_0027
       pct numtasks pending running complete killed failed_attempts
map      1        1       0       0        1      0               0
       killed_attempts
map                  0
 [ reached getOption(&quot;max.print&quot;) -- omitted 1 row ]
Waiting 5 seconds
</code></pre>

<pre><code>* Getting basic &#39;ddo&#39; attributes...
Read 1 objects(1.56 KB) in 0.02 seconds
</code></pre>

<p>Note that this requires a complete read and write of your data.  You should only worry about doing this if you absolutely need random access by key.  The only major requirement for this outside of your own purposes is for use in Trelliscope.</p>

<p>Let&#39;s try to get that subset by key again:</p>

<pre><code class="r">irisDdf[[&quot;key1&quot;]]
</code></pre>

<pre><code>   Sepal.Length Sepal.Width Petal.Length Petal.Width Species
1           5.1         3.5          1.4         0.2  setosa
2           4.9         3.0          1.4         0.2  setosa
 [ reached getOption(&quot;max.print&quot;) -- omitted 38 rows ]
</code></pre>

<h4>MapReduce Example</h4>

<p>Here we again find the top 5 <code>iris</code> records according to sepal width.</p>

<pre><code class="r"># map returns top 5 rows according to sepal width
top5map &lt;- expression({
   counter(&quot;map&quot;, &quot;mapTasks&quot;, 1)
   v &lt;- do.call(rbind, map.values)
   collect(&quot;top5&quot;, v[order(v$Sepal.Width, decreasing=TRUE)[1:5],])
})

# reduce collects map results and then iteratively rbinds them and returns top 5
top5reduce &lt;- expression(
   pre = {
      top5 &lt;- NULL
   }, reduce = {
      top5 &lt;- rbind(top5, do.call(rbind, reduce.values))
      top5 &lt;- top5[order(top5$Sepal.Width, decreasing=TRUE)[1:5],]
   }, post = {
      collect(reduce.key, top5)
   }
)

# execute the job
top5 &lt;- mrExec(bySpecies, map = top5map, reduce = top5reduce)
</code></pre>

<pre><code>* Attempting to create directory... success
* Saving connection attributes
* To initialize the data in this directory as a distributed data object or data frame, call ddo() or ddf()
Saving 1 paramater to /tmp/rhipe-temp-params-ea95f204a6636f593777a1aed7d274d6 (use rhclean to delete all temp files)
</code></pre>

<pre><code>[Sat Nov  9 11:55:58 2013] Name:2013-11-09 11:55:58 Job: job_201311091036_0028  State: PREP Duration: 0.078
URL: http://localhost:50030/jobdetails.jsp?jobid=job_201311091036_0028
       pct numtasks pending running complete killed failed_attempts
map      0        0       0       0        0      0               0
reduce   0        0       0       0        0      0               0
       killed_attempts
map                  0
reduce               0
Waiting 5 seconds
[Sat Nov  9 11:56:03 2013] Name:2013-11-09 11:55:58 Job: job_201311091036_0028  State: RUNNING Duration: 5.136
URL: http://localhost:50030/jobdetails.jsp?jobid=job_201311091036_0028
       pct numtasks pending running complete killed failed_attempts
map      1        1       0       0        1      0               0
reduce   0        1       0       1        0      0               0
       killed_attempts
map                  0
reduce               0
Waiting 5 seconds
[Sat Nov  9 11:56:08 2013] Name:2013-11-09 11:55:58 Job: job_201311091036_0028  State: RUNNING Duration: 10.201
URL: http://localhost:50030/jobdetails.jsp?jobid=job_201311091036_0028
       pct numtasks pending running complete killed failed_attempts
map      1        1       0       0        1      0               0
reduce   0        1       0       1        0      0               0
       killed_attempts
map                  0
reduce               0
Waiting 5 seconds
</code></pre>

<pre><code>* Getting basic &#39;ddo&#39; attributes...
Read 1 objects(0.43 KB) in 0.02 seconds
Read 1 objects(0.43 KB) in 0.02 seconds
</code></pre>

<pre><code class="r"># get the result
top5[[1]]
</code></pre>

<pre><code>[[1]]
[1] &quot;top5&quot;

[[2]]
   Sepal.Length Sepal.Width Petal.Length Petal.Width
16          5.7         4.4          1.5         0.4
34          5.5         4.2          1.4         0.2
33          5.2         4.1          1.5         0.1
15          5.8         4.0          1.2         0.2
6           5.4         3.9          1.7         0.4
</code></pre>

<h4>Working with existing data</h4>

<p>With the RHIPE backend, we are typically analyzing very large data sets that came from somewhere else.  Often this means that the data is already somewhere in the Hadoop ecosystem (Hive table, HDFS text file, etc.).  The easiest way to read data in from another source is when the source is a text file.  We can use RHIPE to parse the lines of text and create R objects.  </p>

<p>To create a scenario where we have text files on HDFS, let&#39;s write the <code>adult</code> dataset as a csv file and move it to HDFS:</p>

<pre><code class="r"># write adult data to csv file
write.table(adult, 
   row.names=FALSE, col.names=FALSE, 
   sep=&quot;,&quot;, quote=FALSE, 
   file=&quot;/tmp/adult.csv&quot;)
</code></pre>

<p>To move the file to HDFS, we call the system <code>hadoop fs -copyFromLocal</code> command to specify that we want to move a local file to HDFS.  We will put the csv file in a directory <code>adult_raw_data</code>:</p>

<pre><code class="r"># create /tmp/adult_raw_data directory on HDFS
rhmkdir(&quot;/tmp/adult_raw_data&quot;)
</code></pre>

<pre><code>[1] TRUE
</code></pre>

<pre><code class="r"># copy the csv from local disk to this directory on HDFS
system(&quot;hadoop fs -copyFromLocal /tmp/adult.csv /tmp/adult_raw_data/adult.csv&quot;)
# make sure it is there
rhls(&quot;/tmp/adult_raw_data&quot;)
</code></pre>

<pre><code>  permission   owner      group     size          modtime
1 -rw-r--r-- hafe647 supergroup 3.418 mb 2013-11-09 11:56
                           file
1 /tmp/adult_raw_data/adult.csv
</code></pre>

<p>Now, we can initiate a &quot;ddo&quot; connection to this data by specifying that <code>type=&quot;text&quot;</code>:</p>

<pre><code class="r"># connect to the csv file on HDFS
adultConn &lt;- hdfsConn(&quot;/tmp/adult_raw_data&quot;, type=&quot;text&quot;)
</code></pre>

<pre><code>* Saving connection attributes
* To initialize the data in this directory as a distributed data object or data frame, call ddo() or ddf()
</code></pre>

<pre><code class="r"># initialize ddo object
adultDdo &lt;- ddo(adultConn)
</code></pre>

<pre><code>* Getting basic &#39;ddo&#39; attributes...
</code></pre>

<pre><code class="r"># look at a key-value pair
adultDdo[[1]]
</code></pre>

<pre><code>[[1]]
[1] &quot;&quot;

[[2]]
[1] &quot;39,State-gov,77516,Bachelors,13,Never-married,Adm-clerical,Not-in-family,White,Male,2174,0,40,United-States,&lt;=50K,0&quot;
</code></pre>

<p>We see that a character string of one line of the csv file is one key-value pair, with the key being blank.</p>

<p>We can cast this data as a &quot;ddf&quot; object by specifying a transformation function that parses the string and turns it into a data frame:</p>

<pre><code class="r"># transformation function to turn line of csv text into data frame
adult2df &lt;- function(line) {
   read.table(textConnection(line), sep=&quot;,&quot;, 
      header=FALSE,
      col.names=c(&quot;age&quot;, &quot;workclass&quot;, &quot;fnlwgt&quot;, &quot;education&quot;, &quot;educationnum&quot;, 
         &quot;marital&quot;, &quot;occupation&quot;, &quot;relationship&quot;, &quot;race&quot;, &quot;sex&quot;, &quot;capgain&quot;, 
         &quot;caploss&quot;, &quot;hoursperweek&quot;, &quot;nativecountry&quot;, &quot;income&quot;, &quot;incomebin&quot;),
         stringsAsFactors=FALSE
   )
}
</code></pre>

<p>Note that it may be necessary to specify column types using the <code>colClasses</code> argument in many cases.</p>

<p>We can specify this transformation function as the <code>transFn</code> argument to <code>ddf()</code> to obtain a data frame representation of the data:</p>

<pre><code class="r">adultDdf &lt;- ddf(adultConn, transFn=adult2df)
</code></pre>

<pre><code>* Reading in existing &#39;ddo&#39; attributes
* Getting basic &#39;ddf&#39; attributes...
</code></pre>

<p>We can see what a transformed key-value pair looks like with:</p>

<pre><code class="r">kvExample(adultDdf, transform=TRUE)
</code></pre>

<pre><code>[[1]]
[1] &quot;&quot;

[[2]]
  age workclass fnlwgt education educationnum       marital   occupation
1  39 State-gov  77516 Bachelors           13 Never-married Adm-clerical
   relationship  race  sex capgain caploss hoursperweek nativecountry
1 Not-in-family White Male    2174       0           40 United-States
  income incomebin
1  &lt;=50K         0
</code></pre>

<p>Now we can use this data to, for example, carry out a division:</p>

<pre><code class="r">byEd &lt;- divide(adultDdf, by=&quot;education&quot;, 
   output=hdfsConn(&quot;/tmp/adultDdf&quot;, autoYes=TRUE))
</code></pre>

<pre><code>* Verifying parameters...
* Applying division...
* Loading connection attributes
Saving 10 paramaters to /tmp/rhipe-temp-params-43f3cc37015609165428d6ddeaec830a (use rhclean to delete all temp files)
</code></pre>

<pre><code>[Sat Nov  9 11:56:20 2013] Name:2013-11-09 11:56:20 Job: job_201311091036_0029  State: PREP Duration: 0.075
URL: http://localhost:50030/jobdetails.jsp?jobid=job_201311091036_0029
       pct numtasks pending running complete killed failed_attempts
map      0        0       0       0        0      0               0
       killed_attempts
map                  0
 [ reached getOption(&quot;max.print&quot;) -- omitted 1 row ]
Waiting 5 seconds
[Sat Nov  9 11:56:25 2013] Name:2013-11-09 11:56:20 Job: job_201311091036_0029  State: RUNNING Duration: 5.1
URL: http://localhost:50030/jobdetails.jsp?jobid=job_201311091036_0029
       pct numtasks pending running complete killed failed_attempts
map      0        1       0       1        0      0               0
       killed_attempts
map                  0
 [ reached getOption(&quot;max.print&quot;) -- omitted 1 row ]
Waiting 5 seconds
[Sat Nov  9 11:56:30 2013] Name:2013-11-09 11:56:20 Job: job_201311091036_0029  State: RUNNING Duration: 10.132
URL: http://localhost:50030/jobdetails.jsp?jobid=job_201311091036_0029
          pct numtasks pending running complete killed failed_attempts
map    0.1891        1       0       1        0      0               0
       killed_attempts
map                  0
 [ reached getOption(&quot;max.print&quot;) -- omitted 1 row ]
Waiting 5 seconds
[Sat Nov  9 11:56:35 2013] Name:2013-11-09 11:56:20 Job: job_201311091036_0029  State: RUNNING Duration: 15.158
URL: http://localhost:50030/jobdetails.jsp?jobid=job_201311091036_0029
          pct numtasks pending running complete killed failed_attempts
map    0.2811        1       0       1        0      0               0
       killed_attempts
map                  0
 [ reached getOption(&quot;max.print&quot;) -- omitted 1 row ]
Waiting 5 seconds
[Sat Nov  9 11:56:40 2013] Name:2013-11-09 11:56:20 Job: job_201311091036_0029  State: RUNNING Duration: 20.179
URL: http://localhost:50030/jobdetails.jsp?jobid=job_201311091036_0029
          pct numtasks pending running complete killed failed_attempts
map    0.3732        1       0       1        0      0               0
       killed_attempts
map                  0
 [ reached getOption(&quot;max.print&quot;) -- omitted 1 row ]
Waiting 5 seconds
[Sat Nov  9 11:56:45 2013] Name:2013-11-09 11:56:20 Job: job_201311091036_0029  State: RUNNING Duration: 25.199
URL: http://localhost:50030/jobdetails.jsp?jobid=job_201311091036_0029
          pct numtasks pending running complete killed failed_attempts
map    0.5598        1       0       1        0      0               0
       killed_attempts
map                  0
 [ reached getOption(&quot;max.print&quot;) -- omitted 1 row ]
Waiting 5 seconds
[Sat Nov  9 11:56:50 2013] Name:2013-11-09 11:56:20 Job: job_201311091036_0029  State: RUNNING Duration: 30.224
URL: http://localhost:50030/jobdetails.jsp?jobid=job_201311091036_0029
          pct numtasks pending running complete killed failed_attempts
map    0.6518        1       0       1        0      0               0
       killed_attempts
map                  0
 [ reached getOption(&quot;max.print&quot;) -- omitted 1 row ]
Waiting 5 seconds
[Sat Nov  9 11:56:55 2013] Name:2013-11-09 11:56:20 Job: job_201311091036_0029  State: RUNNING Duration: 35.241
URL: http://localhost:50030/jobdetails.jsp?jobid=job_201311091036_0029
         pct numtasks pending running complete killed failed_attempts
map    0.836        1       0       1        0      0               0
       killed_attempts
map                  0
 [ reached getOption(&quot;max.print&quot;) -- omitted 1 row ]
Waiting 5 seconds
[Sat Nov  9 11:57:00 2013] Name:2013-11-09 11:56:20 Job: job_201311091036_0029  State: RUNNING Duration: 40.258
URL: http://localhost:50030/jobdetails.jsp?jobid=job_201311091036_0029
       pct numtasks pending running complete killed failed_attempts
map      1        1       0       1        0      0               0
       killed_attempts
map                  0
 [ reached getOption(&quot;max.print&quot;) -- omitted 1 row ]
Waiting 5 seconds
[Sat Nov  9 11:57:05 2013] Name:2013-11-09 11:56:20 Job: job_201311091036_0029  State: RUNNING Duration: 45.278
URL: http://localhost:50030/jobdetails.jsp?jobid=job_201311091036_0029
       pct numtasks pending running complete killed failed_attempts
map      1        1       0       0        1      0               0
       killed_attempts
map                  0
 [ reached getOption(&quot;max.print&quot;) -- omitted 1 row ]
Waiting 5 seconds
[Sat Nov  9 11:57:10 2013] Name:2013-11-09 11:56:20 Job: job_201311091036_0029  State: RUNNING Duration: 50.297
URL: http://localhost:50030/jobdetails.jsp?jobid=job_201311091036_0029
       pct numtasks pending running complete killed failed_attempts
map      1        1       0       0        1      0               0
       killed_attempts
map                  0
 [ reached getOption(&quot;max.print&quot;) -- omitted 1 row ]
Waiting 5 seconds
[Sat Nov  9 11:57:15 2013] Name:2013-11-09 11:56:20 Job: job_201311091036_0029  State: RUNNING Duration: 55.378
URL: http://localhost:50030/jobdetails.jsp?jobid=job_201311091036_0029
       pct numtasks pending running complete killed failed_attempts
map      1        1       0       0        1      0               0
       killed_attempts
map                  0
 [ reached getOption(&quot;max.print&quot;) -- omitted 1 row ]
Waiting 5 seconds
</code></pre>

<pre><code>* Getting basic &#39;ddo&#39; attributes...
Read 1 objects(56.14 KB) in 0.02 seconds
</code></pre>

<p>It is not optimal to operate on one line at a time like this, and we are working on a simple solution that will batches of lines to be read in instead of individually.</p>

<!-- adultDdf <- rhText2df(adultDdo, transFn=readAdult, linesPerBlock=5000, output=hdfsConn("/tmp/adultDdf", autoYes=TRUE)) -->

<h4><code>control</code> options</h4>

<p>For fine control over different parameters of a RHIPE / Hadoop job (and there are many parameters), we use the <code>control</code> argument to any of the <code>datadr</code> functions providing MapReduce functionality (<code>divide()</code>, <code>mrExec()</code>, etc.).  </p>

<p>We can set RHIPE control parameters with the function <code>rhipeControl()</code>, which creates a named list of parameters and their values.  If a parameter isn&#39;t explicitly specified, its default is used.  The parameters available are:</p>

<ul>
<li><code>mapred</code></li>
<li><code>setup</code></li>
<li><code>combiner</code></li>
<li><code>cleanup</code></li>
<li><code>orderby</code></li>
<li><code>shared</code></li>
<li><code>jarfiles</code></li>
<li><code>zips</code></li>
<li><code>jobname</code></li>
</ul>

<p>See the documentation for the RHIPE function <code>rhwatch</code> for details about these:</p>

<pre><code class="r">?rhwatch
</code></pre>

<p>The first three parameters in the list are the most important and often-used, particularly <code>mapred</code>, which is a list specifying specific Hadoop parameters such as <code>mapred.reduce.tasks</code> which can help tune a job.</p>

<p>Defaults for these can be seen by calling <code>rhipeControl()</code> with no arguments.:</p>

<pre><code class="r">rhipeControl()
</code></pre>

<pre><code>$mapred
NULL

$setup
NULL

$combiner
[1] FALSE

$cleanup
NULL

$orderby
[1] &quot;bytes&quot;

$shared
NULL

$jarfiles
NULL

$zips
NULL

$jobname
[1] &quot;&quot;

attr(,&quot;class&quot;)
[1] &quot;rhipeControl&quot;
</code></pre>

</div>


<div class='tab-pane' id='conversion'>
<h3>Conversion</h3>

<p>In many cases, it is useful to be able to convert from one key-value backend to another.  For example, we might have some smaller data out on HDFS that we would like to move to local disk.  Or we might have in-memory data that is looking too large and we want to take advantage of parallel processing so we want to push it to local disk or HDFS.</p>

<p>We can convert data from one backend to another using the <code>convert()</code> method.  The general syntax is <code>convert(from, to)</code> where <code>from</code> is a &quot;ddo/ddf&quot; data object, and <code>to</code> is a &quot;kvConnection&quot; object.  When <code>to=NULL</code>, we are converting to in-memory.</p>

<pre><code class="r"># initialize irisDdf HDFS ddf object
irisDdf &lt;- ddo(hdfsConn(&quot;/tmp/irisKV&quot;))
</code></pre>

<pre><code>* Loading connection attributes
* Reading in existing &#39;ddo&#39; attributes
</code></pre>

<pre><code class="r"># convert from HDFS to in-memory ddf
irisDdfMem &lt;- convert(from=irisDdf)
</code></pre>

<pre><code>Read 3 objects(5.81 KB) in 0.02 seconds
</code></pre>

<pre><code class="r"># convert from HDFS to local disk ddf
irisDdfDisk &lt;- convert(from=irisDdf, 
   to=localDiskConn(&quot;/private/tmp/irisKVdisk&quot;, autoYes=TRUE))
</code></pre>

<pre><code>* Attempting to create directory ... ok
* Saving connection attributes
* Directory is empty - use addData() to add k/v pairs to this directory
</code></pre>

<p>All possible conversions (disk -&gt; HDFS, disk -&gt; memory, HDFS -&gt; disk, HDFS -&gt; memory, memory -&gt; disk, memory -&gt; HDFS) have <code>convert()</code> methods implemented.</p>

</div>


<div class='tab-pane' id='debugging'>
<h3>Debugging</h3>

<p>More to come here, but for now, general guidelines:</p>

<ul>
<li>Get it right first on a subset.  When using the <code>divide()</code> and <code>recombine()</code> interface, pretty much the only place you can introduce errors is in your custom <code>apply</code> function or transformation functions.</li>
<li>With large datasets, read in a small collection of subsets and test those in-memory by calling the same methods on the in-memory object.  <code>browser()</code> is your friend - you can stick this in any user-defined function or inside your map and reduce expressions, which allows you to step into the environment in which your code is operating.</li>
</ul>

<!-- Will be adding capability to set `debug=TRUE` in your `control()` method, in which case whenever there is an error, the key is returned or something along those lines so you can pull out the troublesome key-value pair and see why it was causing problems. -->

</div>


<div class='tab-pane' id='faq'>
<h3>FAQ</h3>

<h4>What should I do if I have an issue or feature request?</h4>

<p>Please post an issue on <a href="https://github.com/hafen/datadr/issues">github</a>.</p>

<h4>When should I consider using <code>datadr</code>?</h4>

<p>Whenever you have large and/or complex data to analyze.  </p>

<p>Complexity is often more of an issue than size.  Complex data requires great flexibility.  We need to be able to do more than run numerical linear algebra routines against the data.  We need to interrogate it from many different angles, both visually and numerically.  <code>datadr</code> strives to provide a very flexible interface for this, while being able to scale.</p>

<h4>What is the state of development of <code>datadr</code>?</h4>

<p><code>datadr</code> started out as proof of concept, and after applying it to several real-world large complex datasets and getting a feel for the requirements, I completely rewrote the package around a more cohesive design, with extensibility in mind.</p>

<p>At this point, we do not anticipate major changes to the interface, but do anticipate many changes under the hood, and perhaps some small changes in how various attributes for data structures are stored and handled.</p>

<h4>What are the plans for future development of <code>datadr</code>?</h4>

<p>Currently the plan is to continue to use the package in applied situations and refine, tweak, and tune performance.  We also plan to continue to add features, and particularly to investigate new backends, such as distributed memory architectures.</p>

<h4>Can you support backend &quot;x&quot; please?</h4>

<p>I am definitely interested in making <code>datadr</code> work with the latest technology.  I&#39;m particularly interested in efficient, scalable, fault-tolerant backends.</p>

<p>If the proposed backend meets these requirements, it is a candidate:</p>

<ul>
<li>data is stored in a key-value store</li>
<li>MapReduce is a feasible computation approach</li>
<li>data can be accessed by key</li>
</ul>

<p>If it has these additional characteristics, it is all the more interesting:</p>

<ul>
<li>it is scalable</li>
<li>work has already been done on generic R interfaces to this backend</li>
<li>other people use it -- it is not an obscure technology</li>
</ul>

<h4>How is <code>datadr</code> similar to / different from Pig?</h4>

<p>Pig is similar to <code>datadr</code> in that it has a high-level language that abstracts MapReduce computations from the user.  Although I&#39;m sure it is possible to run R code somehow with Pig using UDFs, and that it is also probably possible to reproduce division and recombination functionality in pig, the power of <code>datadr</code> comes from the fact that you never leave the R analysis environment, and that you deal with native R data structures.</p>

<p>I see Pig as more of a data processing, transformation, and tabulation engine than a deep statistical analysis environment.  If you are mostly interested in scalable, high-level data manipulation tool and want a mature product (<code>datadr</code> is new and currently has one developer), then Pig is a good choice.  Another good thing about Pig is that it is tightly integrated into the Hadoop ecosystem.  If you are interested in deep analysis with a whole bunch of statistical tools available, then <code>datadr</code> is probably a better choice.</p>

<h4>How is <code>datadr</code> similar to / different from <code>plyr</code> / <code>dplyr</code>?</h4>

<p>Divide and Recombine follows the split/apply/combine paradigm upon which <code>plyr</code> and <code>dplyr</code> are built.  The division is the &quot;split&quot;, and the recombination is the &quot;apply&quot; and &quot;combine&quot;.  <code>datadr</code> is not a replacement or another implementation of these packages, but is an implementation of D&amp;R, and has many unique differences:</p>

<ul>
<li><strong>Persistence of divisions:</strong> In D&amp;R, divisions are new data objects that persist.  This is by design and an important distinction from the <code>plyr</code> implementations.  The reason for persistence is that with very large data, computing a division is expensive compared to recombination, and we don&#39;t want to have to redo the division each time.  We almost always are hitting single a division with hundreds of analytical or visual methods.</li>
<li><strong>Data structures:</strong> The underlying data structure for <code>datadr</code> is key-value pairs, and keys and values can have any data structure.  It is important to have freedom over the use data structures, particularly with very complex data that is difficult to get into a &quot;flattened&quot; state or inconvenient to analyze in such a state.  Some of this is possible with <code>plyr</code>, but <code>dplyr</code> is strictly tabular (according to my understanding).</li>
<li><strong>The nuances of D&amp;R:</strong> There are many D&amp;R-specific ideas implemented in <code>datadr</code> such as random replicate division, between-subset variables, key lookup, etc.</li>
<li><strong>Scale:</strong> Many of the distinctions come down to scale.  D&amp;R was conceived with scaling to multi-terabyte data sets in mind.</li>
</ul>

<h4>How does <code>datadr</code> compare to other R-based big data solutions?</h4>

<p>There are many solutions for dealing with big data in R, so many that I won&#39;t even attempt to make direct comparisons out of fear of making an incorrect assessment of solutions I am not adequately familiar with.</p>

<p>A listing of many approaches can be found at the <a href="http://cran.r-project.org/web/views/HighPerformanceComputing.html">HPC CRAN Task View</a>.  Note that &quot;high-performance computing&quot; does not always mean &quot;big data&quot;.</p>

<p>Instead of making direct comparisons, I will try to point out some things that I think make <code>datadr</code> unique:</p>

<ul>
<li><code>datadr</code> leverages Hadoop, which is routinely used for extremely large data sets</li>
<li><code>datadr</code> as an interface is extensible to other backends</li>
<li><code>datadr</code> is not only a technology linking R to distributed computing backends, but an implementation of D&amp;R, an <em>approach</em> to data analysis that has been used successfully for many analyses of large, complex data</li>
<li><code>datadr</code> provides a backend to scalable detailed visualization of data using <a href="http://hafen.github.io/trelliscope/">trelliscope</a></li>
</ul>

</div>


<div class='tab-pane' id='r-code'>
<h3>R Code</h3>

<p>If you would like to run through all of the code examples in this documentation without having to pick out each line of code from the text, below are files with the R code for each section.</p>

<ul>
<li><a href="docs/code/2data.R">Dealing with Data in D&amp;R</a></li>
<li><a href="docs/code/3dnr.R">Division and Recombination</a></li>
<li><a href="docs/code/4mr.R">MapReduce</a></li>
<li><a href="docs/code/5divag.R">Division Agnostic Methods</a></li>
<li><a href="docs/code/6backend.R">Store/Compute Backends</a></li>
</ul>

</div>

   
   
   <ul class="pager">
      <li><a href="#" id="previous">&larr; Previous</a></li> 
      <li><a href="#" id="next">Next &rarr;</a></li> 
   </ul>
</div>


</div>
</div>

<hr>

<div class="footer">
   <p>&copy; Ryan Hafen, 2013</p>
</div>
</div> <!-- /container -->

<script src="js/jquery.js"></script>
<script src="bootstrap/js/bootstrap.js"></script>
<script src="js/jquery.ba-hashchange.min.js"></script>
<script>
function manageNextPrev() {
   $('a#next').parent().toggleClass('disabled', $('#toc.nav li.active').nextAll('li:not(.nav-header)').size() == 0);
   $('a#previous').parent().toggleClass('disabled', $('#toc.nav li.active').prevAll('li:not(.nav-header)').size() == 0);
   $("html, body").animate({ scrollTop: 0 }, "fast");
}
manageNextPrev();

$('a#next').click(function(e) {
   e.preventDefault();
   location.href = $('#toc.nav li.active').nextAll('li:not(.nav-header)').first().find('a').attr('href');
   manageNextPrev();
});
$('a#previous').click(function(e) {
   e.preventDefault();
   location.href = $('#toc.nav li.active').prevAll('li:not(.nav-header)').first().find('a').attr('href');
   manageNextPrev();
});

$(window).hashchange(function() {
  $('.tab-pane').hide();
  var tab = location.hash || '#background';
  $(tab + '.tab-pane').show();

  $('#toc.nav li.active').removeClass('active');
  $('#toc.nav li a[href="' + tab + '"]').parent().addClass('active');
  manageNextPrev();
});
$(window).hashchange();

$('li.nav-header').dblclick(function(e) {
   var url = $(this).data('edit-href');
   if(url!="")
      window.open(url, '', '');
   return false;
});

</script>
</body>
</html>